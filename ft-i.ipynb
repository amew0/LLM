{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import inspect\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import fire\n",
    "import huggingface_hub\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from utils.eval_helper import inspectt, logg\n",
    "from utils.ft_helper import (\n",
    "    generate_and_tokenize_prompt,\n",
    "    get_start_index,\n",
    "    reorder_dataset,\n",
    ")\n",
    "from torch.utils.data import SequentialSampler\n",
    "wandb.require(\"core\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir: str = f\"/dpc/kunf0097/l3-8b\"\n",
    "train_data_path: str = \"./data/medical-36-row.json\"\n",
    "# train_data_path: str = \"meher146/medical_llama3_instruct_dataset\"\n",
    "model_name: str = \"facebook/opt-350m\"\n",
    "model_save_path: str = None\n",
    "run_id: str = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "chpt_dir: str = None\n",
    "last_checkpoint: str = None\n",
    "per_device_train_batch_size: int = 4\n",
    "gradient_accumulation_steps: int = 4\n",
    "world_size: int = None\n",
    "local_rank: int = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------  ---------------------------\n",
      "------------------------  ---------------------------\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/kunet.ae/ku5001069/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "if model_save_path is None:\n",
    "    model_save_path = f\"{cache_dir}/model/{model_name}-v{run_id}\"\n",
    "\n",
    "if chpt_dir is None:\n",
    "    chpt_dir = f\"{cache_dir}/chpt/{run_id}\"\n",
    "\n",
    "if os.path.isdir(chpt_dir):\n",
    "    checkpoints = [d for d in os.listdir(chpt_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        last_checkpoint = os.path.join(\n",
    "            chpt_dir, max(checkpoints, key=lambda cp: int(cp.split(\"-\")[-1]))\n",
    "        )\n",
    "\n",
    "# if train_data_path locally exists use it\n",
    "if os.path.exists(train_data_path):\n",
    "    data = load_dataset(\"json\", data_files=train_data_path, split=\"train\")\n",
    "else:\n",
    "    data = load_dataset(train_data_path, split=\"train\")\n",
    "\n",
    "start_index = 0\n",
    "if last_checkpoint is not None:\n",
    "    start_index = get_start_index(last_checkpoint, len(data))\n",
    "\n",
    "# device_map = \"auto\"\n",
    "device_map = {\"\": 0}\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": local_rank}\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "\n",
    "start_index = 0\n",
    "if last_checkpoint is not None:\n",
    "    start_index = get_start_index(last_checkpoint, len(data))\n",
    "\n",
    "# device_map = \"auto\"\n",
    "device_map = {\"\": 0}\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": local_rank}\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "inspectt(inspect.currentframe())\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "start = time()\n",
    "load_dotenv()\n",
    "HF_TOKEN_WRITE = os.getenv(\"HF_TOKEN_WRITE\")\n",
    "huggingface_hub.login(token=HF_TOKEN_WRITE)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=f\"{cache_dir}/model\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=f\"{cache_dir}/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        full_prompt = f\"\"\"</s>system</s>{example['instruction'][i]}</s></s>user</s>{example['input'][i]}</s></s>assistant</s>{example['output'][i]}</s>\"\"\"\n",
    "        output_texts.append(full_prompt)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \"</s>assistant</s>\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=data,\n",
    "    args=train_args,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
