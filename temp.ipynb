{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m vllm.entrypoints.openai.api_server --host 127.0.0.1 --port 8881 --model amew0/Meta-Llama-3-8B-Instruct-v240714045919 --dtype float16 --download-dir /dpc/kunf0097/l3-8b/model & ../ngrok http 8881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnas = \" \\n\\n### 1\\nQ: What is the patient's liver condition based on the provided lab results?\\nM: cirrhosis, chronic hepatitis, liver damage, liver failure\\nA: chronic hepatitis\\n\\n### 2\\nQ: What is the patient's serum albumin level?\\nM: 3.4, 7.54, 11.3, 15.6\\nA: 3.4\\n\\n### 3\\nQ: Which of the following is a recommendation for the patient's treatment?\\nM: avoid fatty diet, take beta blocker, take more sugar cane juice, consult gastroenterologist\\nA: consult gastroenterologist\\n\\n### 4\\nQ: What is the patient's bilirubin level?\\nM: 2.38, 4.88, 17.16, 25.8\\nA: 17.16\\n\\n### 5\\nQ: What is the patient's creatinine level?\\nM: 2.5, 4.88, 7.5, 10.5\\nA: 4.88\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = qnas.find(\"### 1\")\n",
    "\n",
    "qna_section = qnas[start_index:].strip()\n",
    "\n",
    "# Skip the empty string before the first ###\n",
    "qna_blocks = qna_section.split(\"### \")[1:]\n",
    "\n",
    "\n",
    "qnas_formatted = []\n",
    "for block in qna_blocks:\n",
    "    lines = block.strip()\n",
    "    q = lines[lines.find(\"Q: \"):lines.find(\"A: \")].strip()\n",
    "    a = lines[lines.find(\"A: \"):].strip()\n",
    "    \n",
    "    qnas_formatted.append({\"Q\": q, \"A\":a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnas_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "from scipy.stats import pearsonr\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from time import time\n",
    "import gc\n",
    "import json\n",
    "import yaml\n",
    "import argparse\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import fire\n",
    "import inspect\n",
    "\n",
    "\n",
    "logg = lambda x: print(f\"------------------------ {x} ---------------------------\")\n",
    "\n",
    "\n",
    "def inspectt(frame):\n",
    "    logg(\"\")\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    for arg in args:\n",
    "        print(f\"\\t{arg}: {values[arg]}\")\n",
    "    logg(\"\")\n",
    "\n",
    "def get_prompts_from_template(filepath, name, eval_name):\n",
    "    default_config = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "\n",
    "    candidate_prompt = data[name][\"candidate_prompt\"]\n",
    "    evaluator_prompt = data[eval_name][\"evaluator_prompt\"]\n",
    "    candidate_generation_config = data[name].get(\"candidate_generation_config\", default_config)\n",
    "    evaluator_generation_config = data[eval_name].get(\n",
    "        \"evaluator_generation_config\", default_config\n",
    "    )\n",
    "\n",
    "    print(\"candidate_prompt: \", candidate_prompt)\n",
    "    print(\"evaluator_prompt: \", evaluator_prompt)\n",
    "    print(\"candidate_generation_config: \", candidate_generation_config)\n",
    "    print(\"evaluator_generation_config: \", evaluator_generation_config)\n",
    "\n",
    "    return (\n",
    "        candidate_prompt,\n",
    "        evaluator_prompt,\n",
    "        candidate_generation_config,\n",
    "        evaluator_generation_config,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_tokenizer_and_model(model_name: str, cache_dir: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=f\"{cache_dir}/tokenizer\",\n",
    "        pad_token_id=0,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=f\"{cache_dir}/model\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        offload_buffers=True,\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def tokenize(prompt, tokenizer):\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(batch, tokenizer, prompt_template):\n",
    "    # print(batch)\n",
    "    prompts = [prompt_template.format(d[0], d[1]) for d in zip(batch[\"instruction\"], batch[\"input\"])]\n",
    "    print(\"a\", prompts)\n",
    "    tokenized_prompts = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "    print(\"b\", tokenized_prompts)\n",
    "    return tokenized_prompts\n",
    "\n",
    "\n",
    "def eval_prompt_tokenizer(generated, output, eval_tokenizer, prompt=None):\n",
    "    prompt = prompt.format(generated, output)\n",
    "    tokenized_full_prompt = tokenize(prompt, tokenizer=eval_tokenizer)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "\n",
    "def extract_score(text):\n",
    "    match = re.search(r\"\\b\\d+\\.\\d+\\b\", text)\n",
    "    return float(match.group(0)) if match else -1.0\n",
    "\n",
    "\n",
    "def log2json(results, json_result):\n",
    "    with open(json_result, \"w\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def generate_response(model, tokenizer, input_ids, attention_mask, generation_config):\n",
    "    # torch.LongTensor(input_ids).to(model.device)\n",
    "    # torch.LongTensor(attention_mask).to(model.device)\n",
    "    # try:\n",
    "        output = model.generate(\n",
    "            input_ids=torch.stack(input_ids).to(model.device),\n",
    "            attention_mask=torch.stack(attention_mask).to(model.device),\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            **generation_config,\n",
    "        )\n",
    "        print(output[0])\n",
    "        response_ids = output[0][len(input_ids[0]) :]\n",
    "        response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "        return response, output\n",
    "    # except RuntimeError as e:\n",
    "    #     if \"inf\" in str(e) or \"nan\" in str(e):\n",
    "    #         print(f\"Skipping example due to invalid output: {e}\")\n",
    "    #         return None\n",
    "    #     else:\n",
    "    #         raise \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=f\"./out\"\n",
    "cache_dir=f\"/dpc/kunf0097/l3-8b\"\n",
    "eval_data_path=\"./data/1/eval_sample.json\"\n",
    "log_file=None\n",
    "name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "eval_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "run_id=datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "log2wandb: bool = True\n",
    "project=\"huggingface\"\n",
    "entity=\"my-ku-org\"\n",
    "evals_per_example=2\n",
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "evaluator_name=\"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    candidate_prompt,\n",
    "    evaluator_prompt,\n",
    "    candidate_generation_config,\n",
    "    evaluator_generation_config,\n",
    ") = get_prompts_from_template(\"template.yaml\", candidate_name, evaluator_name)\n",
    "\n",
    "\n",
    "if log2wandb and (project is None or entity is None):\n",
    "    raise ValueError(\"Both 'project' and 'entity' must be set if 'log2wandb' is True.\")\n",
    "\n",
    "if log_file is None:\n",
    "    log_file = f\"{output_dir}/results_{name.split('/')[1]}_{run_id}.json\"\n",
    "\n",
    "inspectt(inspect.currentframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator_tokenizer, evaluator_model = get_tokenizer_and_model(\n",
    "#     model_name=eval_name, cache_dir=cache_dir\n",
    "# )\n",
    "\n",
    "candidate_tokenizer, candidate_model = get_tokenizer_and_model(\n",
    "    model_name=name, cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# candidate_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#         name,\n",
    "#         cache_dir=f\"{cache_dir}/tokenizer\",\n",
    "#         pad_token_id=0,\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_tokenizer.pad_token = candidate_tokenizer.bos_token \n",
    "candidate_tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=eval_data_path)\n",
    "eval_dataset = data[\"train\"].map(\n",
    "    lambda x: generate_and_tokenize_prompt(x, candidate_tokenizer, candidate_prompt),\n",
    "    batched=True,  # Process in batches\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_eval_dataset  =torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batched_eval_dataset :\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(batch[\"input_ids\"])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(batch[\"attention_mask\"])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidate_tokenizer.decode(torch.stack(batch[\"input_ids\"])[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, output = generate_response(\n",
    "    candidate_model,\n",
    "    candidate_tokenizer,\n",
    "    batch[\"input_ids\"],\n",
    "    batch[\"attention_mask\"],\n",
    "    candidate_generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidate_tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enshiallah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"out/results_240623023136_240628153415.json\", \"r\") as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "table = wandb.Table(columns=list(results[0].keys()))\n",
    "for r in results:\n",
    "    table.add_data(*r.values())\n",
    "run =wandb.init(project=\"huggingface\", entity=\"my-ku-org\", name=\"laaj-llama-3-8b-medical-v240623023136\")\n",
    "wandb.log({\"Evaluation Results\": table})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evals **MMLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =load_dataset(\"cais/mmlu\", \"clinical_knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = data[\"dev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import inspect\n",
    "import os\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import fire\n",
    "import huggingface_hub\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from utils.eval_helper import inspectt, logg\n",
    "from transformers import TrainerCallback\n",
    "from utils.ft_helper import generate_and_tokenize_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./out\"\n",
    "cache_dir = f\"/dpc/kunf0097/l3-8b\"\n",
    "train_data_path = \"./data/medical-36-row.json\"\n",
    "model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_save_path: str = None\n",
    "chpt_dir: str = None\n",
    "run_id = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "# run_id = \"240714045919\"\n",
    "prompt_template=None\n",
    "\n",
    "if model_save_path is None:\n",
    "    model_save_path = f\"{cache_dir}/model/{model_name}-v{run_id}\"\n",
    "if chpt_dir is None:\n",
    "    chpt_dir = f\"{cache_dir}/chpt/{run_id}\"\n",
    "\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(chpt_dir):\n",
    "    checkpoints = [d for d in os.listdir(chpt_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        last_checkpoint = os.path.join(\n",
    "            chpt_dir, max(checkpoints, key=lambda cp: int(cp.split(\"-\")[-1]))\n",
    "        )\n",
    "import yaml\n",
    "if prompt_template is None:\n",
    "        with open(\"tuning.yaml\", \"r\") as f:\n",
    "            tuning_config = yaml.safe_load(f)\n",
    "            prompt_template = tuning_config[model_name][\"prompt_template\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_header_id|>system<|end_header_id|>\\n{}<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\nQuestion: {}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n{}<|eot_id|>\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, tokenizer, cutoff_len: int = None):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    result[\"input_ids\"] = result[\"input_ids\"].flatten()\n",
    "    result[\"attention_mask\"] = result[\"attention_mask\"].flatten()\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].clone()  # Clone input_ids for labels\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_and_tokenize_promptt(\n",
    "    data_point, tokenizer, cutoff_len: int = None, prompt_template: str = None\n",
    "):\n",
    "    train_on_input = False\n",
    "    if cutoff_len is None:\n",
    "        tokenized_full_prompt = tokenize(\n",
    "            prompt_template.format(\n",
    "                data_point[\"instruction\"], data_point[\"input\"], data_point[\"output\"]\n",
    "            ),\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "    else:\n",
    "        tokenized_full_prompt = tokenize(\n",
    "            prompt_template.format(\n",
    "                data_point[\"instruction\"], data_point[\"input\"], data_point[\"output\"]\n",
    "            ),\n",
    "            tokenizer=tokenizer,\n",
    "            cutoff_len=cutoff_len,\n",
    "        )\n",
    "        if not train_on_input:\n",
    "            prompt_template = prompt_template.split(\n",
    "                \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "            )[0]\n",
    "            tokenized_user_prompt = tokenize(\n",
    "                prompt_template.format(data_point[\"instruction\"], data_point[\"input\"]),\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "            labels_prefix = torch.full((user_prompt_len,), -100)\n",
    "            tokenized_full_prompt[\"labels\"] = torch.cat(\n",
    "                (\n",
    "                    labels_prefix,\n",
    "                    torch.tensor(tokenized_full_prompt[\"labels\"][user_prompt_len:]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=f\"{cache_dir}/tokenizer\")\n",
    "\n",
    "# # Initialize model\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     cache_dir=f\"{cache_dir}/model\",\n",
    "#     # quantization_config=bnb_config,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     # return_dict=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for LoRA training\n",
    "peft_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def reorder_dataset(dataset, start_index):\n",
    "    # Split the dataset into two parts: before and after the start index\n",
    "    dataset_part1 = dataset.select(range(start_index, len(dataset)))\n",
    "    dataset_part2 = dataset.select(range(start_index))\n",
    "    \n",
    "    # Concatenate the two parts to get the reordered dataset\n",
    "    reordered_dataset = concatenate_datasets([dataset_part1, dataset_part2])\n",
    "    return reordered_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04719cbb9d5442484d814aae8653857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n",
      "105\n",
      "95\n",
      "97\n",
      "98\n",
      "124\n",
      "121\n",
      "113\n",
      "115\n",
      "146\n",
      "127\n",
      "119\n",
      "91\n",
      "131\n",
      "126\n",
      "98\n",
      "235\n",
      "130\n",
      "111\n",
      "109\n",
      "138\n",
      "101\n",
      "133\n",
      "251\n",
      "129\n",
      "108\n",
      "100\n",
      "141\n",
      "98\n",
      "102\n",
      "125\n",
      "125\n",
      "133\n",
      "138\n",
      "155\n",
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurm-ku5001069@kunet.ae-1140545/ipykernel_67910/2056480202.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(tokenized_full_prompt[\"labels\"][user_prompt_len:]),\n"
     ]
    }
   ],
   "source": [
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 1\n",
    "start_index = 0\n",
    "if last_checkpoint is not None:\n",
    "    start_index = (\n",
    "        int(last_checkpoint.split(\"-\")[-1])\n",
    "        * per_device_train_batch_size\n",
    "        * gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "data = load_dataset(\"json\", data_files=train_data_path, split=\"train\")\n",
    "# data = reorder_dataset(data, start_index)\n",
    "\n",
    "cutoff_len = 296  # (75% of the data wont be affected)\n",
    "train_dataset = data.map(lambda x: generate_and_tokenize_promptt(x, tokenizer, cutoff_len, prompt_template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_header_id|>assistant<|end_header_id|>\\nHi, Thank you for posting your query. The most likely cause for your symptoms is benign paroxysmal positional vertigo (BPPV), a type of peripheral vertigo. In this condition, the most common symptom is dizziness or giddiness, which is made worse with movements. Accompanying nausea and vomiting are common. The condition is due to problem in the ear, and improves in a few days on own. Betahistine tablets would help relieve your symptoms. Doing vestibular rehabilitation or adaptation exercises would prevent the recurrence of these symptoms. An ENT evaluation would also help. I hope it helps.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([i for i in train_dataset[0][\"labels\"] if i != -100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it from .yaml\n",
    "train_args = SFTConfig(\n",
    "    run_name=f\"ft-{model_name.split('/')[1]}-{run_id}-v{start_index}\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # only 1 is allowed on the no shuffler [needs revision]\n",
    "    eval_accumulation_steps=1,  # !very important to send data to cpu\n",
    "    warmup_steps=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=False,\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_torch\",\n",
    "    output_dir=f\"{chpt_dir}\",\n",
    "    group_by_length=False,\n",
    "    dataloader_drop_last=False,\n",
    "    save_steps=2,\n",
    "    save_total_limit=3,\n",
    "    max_seq_length=cutoff_len,\n",
    "    resume_from_checkpoint=last_checkpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "import datasets\n",
    "from transformers.trainer_utils import seed_worker\n",
    "\n",
    "\n",
    "class SFTTrainerNoShuffle(SFTTrainer):\n",
    "    def training_step(self, model, inputs):\n",
    "        if (self.state.global_step % self.args.save_steps) == 0:\n",
    "            inputs_decoded = tokenizer.decode(inputs[\"input_ids\"][0])\n",
    "            logger.info(f\"{self.state.global_step}: {inputs_decoded}\")\n",
    "        return super().training_step(model, inputs)\n",
    "\n",
    "    def _get_train_sampler(self):\n",
    "        return SequentialSampler(self.train_dataset)  # to prevent shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:peft.tuners.tuners_utils:Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainerNoShuffle(\n",
    "# trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    args=train_args,\n",
    "    # callbacks=[PrintExampleCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval / Chpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftmodel = PeftModel.from_pretrained(model, last_checkpoint)\n",
    "ftmodel = ftmodel.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7815b30bc81b421f89e91d04b6ab8893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def eval_tokenize(prompt, tokenizer):\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def eval_generate_and_tokenize_prompt(data_point, tokenizer, prompt=None):\n",
    "    prompt = prompt.format(data_point[\"instruction\"], data_point[\"input\"])\n",
    "    tokenized_full_prompt = eval_tokenize(prompt, tokenizer=tokenizer)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "prompt_template = \"<|start_header_id|>system<|end_header_id|> {}<|eot_id|><|start_header_id|>user<|end_header_id|> This is the question: {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "eval_split = \"train\"\n",
    "data = load_dataset(\"json\", data_files=train_data_path, split=eval_split)\n",
    "eval_dataset = data.map(\n",
    "    lambda x: eval_generate_and_tokenize_prompt(x, tokenizer, prompt_template)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> If you are a doctor, please answer the medical questions based on the patient's description.<|eot_id|><|start_header_id|>user<|end_header_id|> This is the question: I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.. Thank you doc!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from utils.eval_helper import generate_response\n",
    "\n",
    "response = generate_response(\n",
    "    ftmodel,\n",
    "    tokenizer,\n",
    "    example[\"input_ids\"],\n",
    "    example[\"attention_mask\"],\n",
    "    {},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello, The symptoms as mentioned in your post can be attributed to a condition known as Benign Paroxysmal Positional Vertigo (BPPV). It is a disorder of the vestibular apparatus in the inner ear, in which episodes of vertigo are triggered by changes in the position of the head. The symptoms you mention in your post can be attributed to BPPV. I would like to suggest you to consult a Neurologist/ ENT Specialist for a complete clinical examination & relevant investigations. Investigations will be needed to confirm the diagnosis. There is a good chance of it getting cured with specific therapy. Till then, you can follow these measures'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello, I understand your concern. I am Chat Doctor, infectious diseases specialist, answering your query. In my opinion you should go for complete blood count, and you may need the same. The reason for your symptom is the anemia. It makes you feel dizzy. I will suggest you to take iron supplements for the same. You can take it after consulting your doctor. In my opinion you should visit the nearby hospital as soon as possible. I will be happy to answer your further concern, you can ask me on bit.ly/ Chat Doctor.  Thank you. ChatDoctorInfectious diseases specialist.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuned\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_TOKEN_WRITE = os.environ[\"HF_TOKEN_WRITE\"]\n",
    "tokenizer.push_to_hub(f\"{model_name.split('/')[1]}-v{run_id}_si{start_index}\", token=HF_TOKEN_WRITE)\n",
    "ftmodel.push_to_hub(f\"{model_name.split('/')[1]}-v{run_id}_si{start_index}\", token=HF_TOKEN_WRITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
