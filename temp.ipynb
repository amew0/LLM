{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "from scipy.stats import pearsonr\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from time import time\n",
    "import gc\n",
    "import json\n",
    "import yaml\n",
    "import argparse\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import fire\n",
    "import inspect\n",
    "\n",
    "\n",
    "logg = lambda x: print(f\"------------------------ {x} ---------------------------\")\n",
    "\n",
    "\n",
    "def inspectt(frame):\n",
    "    logg(\"\")\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    for arg in args:\n",
    "        print(f\"\\t{arg}: {values[arg]}\")\n",
    "    logg(\"\")\n",
    "\n",
    "def get_prompts_from_template(filepath, name, eval_name):\n",
    "    default_config = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "\n",
    "    candidate_prompt = data[name][\"candidate_prompt\"]\n",
    "    evaluator_prompt = data[eval_name][\"evaluator_prompt\"]\n",
    "    candidate_generation_config = data[name].get(\"candidate_generation_config\", default_config)\n",
    "    evaluator_generation_config = data[eval_name].get(\n",
    "        \"evaluator_generation_config\", default_config\n",
    "    )\n",
    "\n",
    "    print(\"candidate_prompt: \", candidate_prompt)\n",
    "    print(\"evaluator_prompt: \", evaluator_prompt)\n",
    "    print(\"candidate_generation_config: \", candidate_generation_config)\n",
    "    print(\"evaluator_generation_config: \", evaluator_generation_config)\n",
    "\n",
    "    return (\n",
    "        candidate_prompt,\n",
    "        evaluator_prompt,\n",
    "        candidate_generation_config,\n",
    "        evaluator_generation_config,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_tokenizer_and_model(model_name: str, cache_dir: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=f\"{cache_dir}/tokenizer\",\n",
    "        pad_token_id=0,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=f\"{cache_dir}/model\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        offload_buffers=True,\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def tokenize(prompt, tokenizer):\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(batch, tokenizer, prompt_template):\n",
    "    # print(batch)\n",
    "    prompts = [prompt_template.format(d[0], d[1]) for d in zip(batch[\"instruction\"], batch[\"input\"])]\n",
    "    print(\"a\", prompts)\n",
    "    tokenized_prompts = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "    print(\"b\", tokenized_prompts)\n",
    "    return tokenized_prompts\n",
    "\n",
    "\n",
    "def eval_prompt_tokenizer(generated, output, eval_tokenizer, prompt=None):\n",
    "    prompt = prompt.format(generated, output)\n",
    "    tokenized_full_prompt = tokenize(prompt, tokenizer=eval_tokenizer)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "\n",
    "def extract_score(text):\n",
    "    match = re.search(r\"\\b\\d+\\.\\d+\\b\", text)\n",
    "    return float(match.group(0)) if match else -1.0\n",
    "\n",
    "\n",
    "def log2json(results, json_result):\n",
    "    with open(json_result, \"w\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def generate_response(model, tokenizer, input_ids, attention_mask, generation_config):\n",
    "    # torch.LongTensor(input_ids).to(model.device)\n",
    "    # torch.LongTensor(attention_mask).to(model.device)\n",
    "    # try:\n",
    "        output = model.generate(\n",
    "            input_ids=torch.stack(input_ids).to(model.device),\n",
    "            attention_mask=torch.stack(attention_mask).to(model.device),\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            **generation_config,\n",
    "        )\n",
    "        print(output[0])\n",
    "        response_ids = output[0][len(input_ids[0]) :]\n",
    "        response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "        return response, output\n",
    "    # except RuntimeError as e:\n",
    "    #     if \"inf\" in str(e) or \"nan\" in str(e):\n",
    "    #         print(f\"Skipping example due to invalid output: {e}\")\n",
    "    #         return None\n",
    "    #     else:\n",
    "    #         raise \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=f\"./out\"\n",
    "cache_dir=f\"/dpc/kunf0097/l3-8b\"\n",
    "eval_data_path=\"./data/1/eval_sample.json\"\n",
    "log_file=None\n",
    "name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "eval_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "run_id=datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "log2wandb: bool = True\n",
    "project=\"huggingface\"\n",
    "entity=\"my-ku-org\"\n",
    "evals_per_example=2\n",
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "evaluator_name=\"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate_prompt:  <|start_header_id|>system<|end_header_id|> {}<|eot_id|><|start_header_id|>user<|end_header_id|> {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "evaluator_prompt:  <|start_header_id|>system<|end_header_id|>\n",
      "You are going to act as an LLM evaluator to rate the answer of the LLM generated medical chatbot on factualness (i.e how contextually the generated output followed the expected reply). Penalize it appropriately for any hallucination/loss of context, refraining to answer (saying 'As an AI Language Model', 'I am not a doctor, but' ...), or trailing repetition. YOUR RESPONSE IS NOTHING ELSE BUT A FLOAT FROM 0.0 - 5.0 (with format x.x). Where 0.0 indicates the context of the generated response is very far from the expected one. And 5.0 represents otherwise. AGAIN IF YOUR GENERATED ANYTHING ELSE BUT A FLOAT YOU'RE GOING TO CRUSH MY SYSTEM!!<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "### Expected: {}\n",
      "### Generated: {}<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "candidate_generation_config:  {'max_new_tokens': 256, 'do_sample': True, 'temperature': 0.6, 'top_p': 0.9}\n",
      "evaluator_generation_config:  {'max_new_tokens': 256, 'do_sample': True, 'temperature': 0.6, 'top_p': 0.9}\n",
      "------------------------  ---------------------------\n",
      "------------------------  ---------------------------\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    candidate_prompt,\n",
    "    evaluator_prompt,\n",
    "    candidate_generation_config,\n",
    "    evaluator_generation_config,\n",
    ") = get_prompts_from_template(\"template.yaml\", candidate_name, evaluator_name)\n",
    "\n",
    "\n",
    "if log2wandb and (project is None or entity is None):\n",
    "    raise ValueError(\"Both 'project' and 'entity' must be set if 'log2wandb' is True.\")\n",
    "\n",
    "if log_file is None:\n",
    "    log_file = f\"{output_dir}/results_{name.split('/')[1]}_{run_id}.json\"\n",
    "\n",
    "inspectt(inspect.currentframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d594bd05d29942aeb63b006eb6882a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluator_tokenizer, evaluator_model = get_tokenizer_and_model(\n",
    "#     model_name=eval_name, cache_dir=cache_dir\n",
    "# )\n",
    "\n",
    "candidate_tokenizer, candidate_model = get_tokenizer_and_model(\n",
    "    model_name=name, cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# candidate_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#         name,\n",
    "#         cache_dir=f\"{cache_dir}/tokenizer\",\n",
    "#         pad_token_id=0,\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_tokenizer.pad_token = candidate_tokenizer.bos_token \n",
    "candidate_tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=eval_data_path)\n",
    "eval_dataset = data[\"train\"].map(\n",
    "    lambda x: generate_and_tokenize_prompt(x, candidate_tokenizer, candidate_prompt),\n",
    "    batched=True,  # Process in batches\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_eval_dataset  =torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': [\"If you are a doctor, please answer the medical questions based on the patient's description.\", \"If you are a doctor, please answer the medical questions based on the patient's description.\"], 'input': ['my mothers age 58, jaundice problemcreatinine 4.88, sodium-125, potasium-4.6; chloride-85;bilirubin total-17.16;bilirubin direct-2.38;sgot-155;sgpt-160;alkaline phos-260;protein-7.54;albumin-3.4;urea-138 and BP 30nowplease help. what is status and in which stage she is?', 'Whether heart disease can be cured by Homeopathic treatment. Heart if functioning upto 25 % and when medicine for heart beat improvement is taken then it affects Blood pressures and it gets low and when medicine for improving Blood pressure is teken then heart beat gets low. Twice patient was admitted to Kokilaben Ambani Hospital and he is under treatment of Dr. Jamshed Dalal in last one year. Please suggest whether this heart problem can be cured by taking Homeopathic treqtment. Also please suggest name address of any Doctor at Mumbai who can provide Homeopathic treatment for curing the above disease.'], 'output': ['Hi thanks for contacting Chat Doctor.... Your liver enzymes are high....with increased bilirubin. Your albumin level low ....so you are having chronic liver problem.... You might have cirrhosis or chronic hepatitis.... For grading liver biopsy needed... Your physical examination must be done for splenomegaly and ascites..... If portal hypertension present beta blocker needed... For jaundice fruits taken more.... Excess fatty diet avoided.... Sugar cane juice, apple juice taken more.... Avoid strenuous work.... Consult gastroenterologist for detail examination and further opinion... Take care.... Chat Doctor.', \"Hello Concerned Person, A chronic heart condition responds well to Homeopathy if the patients' vitality is good. Since you have not mentioned the age of the patient or the actual diagnosis your cardiologist has given, it would be wrong to give a definite idea about the scope of Homeopathy in this case. However, there are some wonderful remedies in Homeopathy for heart failure and arrhythmias. I would recommend you to submit a complete case record with reports and then a call can be taken about treatment. You can mail the reports at Chat Doctor. Com if you would like further guidance. All the best!\"], 'prompt': [\"<|start_header_id|>system<|end_header_id|> If you are a doctor, please answer the medical questions based on the patient's description.<|eot_id|><|start_header_id|>user<|end_header_id|> This is the question: my mothers age 58, jaundice problemcreatinine 4.88, sodium-125, potasium-4.6; chloride-85;bilirubin total-17.16;bilirubin direct-2.38;sgot-155;sgpt-160;alkaline phos-260;protein-7.54;albumin-3.4;urea-138 and BP 30nowplease help. what is status and in which stage she is?<|eot_id|><|start_header_id|>assistant<|end_header_id|> Hi thanks for contacting Chat Doctor.... Your liver enzymes are high....with increased bilirubin. Your albumin level low ....so you are having chronic liver problem.... You might have cirrhosis or chronic hepatitis.... For grading liver biopsy needed... Your physical examination must be done for splenomegaly and ascites..... If portal hypertension present beta blocker needed... For jaundice fruits taken more.... Excess fatty diet avoided.... Sugar cane juice, apple juice taken more.... Avoid strenuous work.... Consult gastroenterologist for detail examination and further opinion... Take care.... Chat Doctor.<|eot_id|>\", \"<|start_header_id|>system<|end_header_id|> If you are a doctor, please answer the medical questions based on the patient's description.<|eot_id|><|start_header_id|>user<|end_header_id|> This is the question: Whether heart disease can be cured by Homeopathic treatment. Heart if functioning upto 25 % and when medicine for heart beat improvement is taken then it affects Blood pressures and it gets low and when medicine for improving Blood pressure is teken then heart beat gets low. Twice patient was admitted to Kokilaben Ambani Hospital and he is under treatment of Dr. Jamshed Dalal in last one year. Please suggest whether this heart problem can be cured by taking Homeopathic treqtment. Also please suggest name address of any Doctor at Mumbai who can provide Homeopathic treatment for curing the above disease.<|eot_id|><|start_header_id|>assistant<|end_header_id|> Hello Concerned Person, A chronic heart condition responds well to Homeopathy if the patients' vitality is good. Since you have not mentioned the age of the patient or the actual diagnosis your cardiologist has given, it would be wrong to give a definite idea about the scope of Homeopathy in this case. However, there are some wonderful remedies in Homeopathy for heart failure and arrhythmias. I would recommend you to submit a complete case record with reports and then a call can be taken about treatment. You can mail the reports at Chat Doctor. Com if you would like further guidance. All the best!<|eot_id|>\"], 'input_ids': [tensor([128000, 128000]), tensor([128000, 128006]), tensor([128000,   9125]), tensor([128000, 128007]), tensor([128000,   1442]), tensor([128000,    499]), tensor([128000,    527]), tensor([128000,    264]), tensor([128000,  10896]), tensor([128000,     11]), tensor([128000,   4587]), tensor([128000,   4320]), tensor([128000,    279]), tensor([128000,   6593]), tensor([128000,   4860]), tensor([128000,   3196]), tensor([128006,    389]), tensor([9125,  279]), tensor([128007,   8893]), tensor([1442,  596]), tensor([ 499, 4096]), tensor([527,  13]), tensor([   264, 128009]), tensor([ 10896, 128006]), tensor([ 11, 882]), tensor([  4587, 128007]), tensor([ 4320, 13440]), tensor([ 279, 4851]), tensor([6593, 8624]), tensor([4860,  649]), tensor([3196,  387]), tensor([  389, 64688]), tensor([279, 555]), tensor([8893, 5492]), tensor([  596, 62209]), tensor([4096, 6514]), tensor([13, 13]), tensor([128009,  18449]), tensor([128006,    422]), tensor([  882, 31301]), tensor([128007,  81226]), tensor([856, 220]), tensor([27698,   914]), tensor([4325, 1034]), tensor([220, 323]), tensor([2970,  994]), tensor([   11, 16088]), tensor([12203,   369]), tensor([1263, 4851]), tensor([ 560, 9567]), tensor([ 3575, 16048]), tensor([846, 374]), tensor([15111,  4529]), tensor([ 483, 1243]), tensor([220, 433]), tensor([   19, 22223]), tensor([   13, 20671]), tensor([ 2421, 40850]), tensor([ 11, 323]), tensor([39695,   433]), tensor([  12, 5334]), tensor([6549, 3428]), tensor([ 11, 323]), tensor([3419,  994]), tensor([  300, 16088]), tensor([2411,  369]), tensor([   12, 18899]), tensor([   19, 20671]), tensor([  13, 7410]), tensor([ 21, 374]), tensor([  26, 1028]), tensor([82882,  2779]), tensor([  12, 1243]), tensor([5313, 4851]), tensor([  26, 9567]), tensor([112474,   5334]), tensor([ 392, 3428]), tensor([258,  13]), tensor([ 2860, 94666]), tensor([  12, 8893]), tensor([1114,  574]), tensor([   13, 16584]), tensor([845, 311]), tensor([   26, 65815]), tensor([112474,    321]), tensor([  392, 41408]), tensor([  258, 20423]), tensor([2167, 5676]), tensor([   12, 15429]), tensor([ 17, 323]), tensor([ 13, 568]), tensor([1987,  374]), tensor([  26, 1234]), tensor([2034, 6514]), tensor([354, 315]), tensor([  12, 2999]), tensor([9992,   13]), tensor([   26, 20614]), tensor([ 2034, 70561]), tensor([  418, 29210]), tensor([ 12, 278]), tensor([6330,  304]), tensor([  26, 1566]), tensor([1727,  832]), tensor([ 278, 1060]), tensor([483,  13]), tensor([1343, 5321]), tensor([ 437, 4284]), tensor([  12, 3508]), tensor([11387,   420]), tensor([  26, 4851]), tensor([79565,  3575]), tensor([ 12, 649]), tensor([ 22, 387]), tensor([   13, 64688]), tensor([4370,  555]), tensor([  26, 4737]), tensor([21687,  5492]), tensor([  258, 62209]), tensor([ 12, 259]), tensor([  18, 3031]), tensor([13, 83]), tensor([ 19, 479]), tensor([26, 13]), tensor([ 554, 7429]), tensor([  64, 4587]), tensor([  12, 4284]), tensor([10350,   836]), tensor([ 323, 2686]), tensor([30167,   315]), tensor([220, 904]), tensor([  966, 19150]), tensor([3409,  520]), tensor([31121, 35812]), tensor([1520,  889]), tensor([ 13, 649]), tensor([1148, 3493]), tensor([ 374, 5492]), tensor([ 2704, 62209]), tensor([ 323, 6514]), tensor([304, 369]), tensor([  902, 97330]), tensor([6566,  279]), tensor([1364, 3485]), tensor([ 374, 8624]), tensor([30, 13]), tensor([128009, 128009]), tensor([128006, 128006]), tensor([78191, 78191]), tensor([128007, 128007])], 'attention_mask': [tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1])]}\n"
     ]
    }
   ],
   "source": [
    "for batch in batched_eval_dataset :\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
       "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128006,   9125,\n",
       "        128007,   1442,    499,    527,    264,  10896,     11,   4587,   4320,\n",
       "           279,   6593,   4860,   3196,    389,    279,   8893,    596,   4096,\n",
       "            13, 128009, 128006,    882, 128007,    856,  27698,   4325,    220,\n",
       "          2970,     11,  12203,   1263,    560,   3575,    846,  15111,    483,\n",
       "           220,     19,     13,   2421,     11,  39695,     12,   6549,     11,\n",
       "          3419,    300,   2411,     12,     19,     13,     21,     26,  82882,\n",
       "            12,   5313,     26, 112474,    392,    258,   2860,     12,   1114,\n",
       "            13,    845,     26, 112474,    392,    258,   2167,     12,     17,\n",
       "            13,   1987,     26,   2034,    354,     12,   9992,     26,   2034,\n",
       "           418,     12,   6330,     26,   1727,    278,    483,   1343,    437,\n",
       "            12,  11387,     26,  79565,     12,     22,     13,   4370,     26,\n",
       "         21687,    258,     12,     18,     13,     19,     26,    554,     64,\n",
       "            12,  10350,    323,  30167,    220,    966,   3409,  31121,   1520,\n",
       "            13,   1148,    374,   2704,    323,    304,    902,   6566,   1364,\n",
       "           374,     30, 128009, 128006,  78191, 128007])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(batch[\"input_ids\"])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(batch[\"attention_mask\"])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|> If you are a doctor, please answer the medical questions based on the patient's description.<|eot_id|><|start_header_id|>user<|end_header_id|> Whether heart disease can be cured by Homeopathic treatment. Heart if functioning upto 25 % and when medicine for heart beat improvement is taken then it affects Blood pressures and it gets low and when medicine for improving Blood pressure is teken then heart beat gets low. Twice patient was admitted to Kokilaben Ambani Hospital and he is under treatment of Dr. Jamshed Dalal in last one year. Please suggest whether this heart problem can be cured by taking Homeopathic treqtment. Also please suggest name address of any Doctor at Mumbai who can provide Homeopathic treatment for curing the above disease.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(candidate_tokenizer.decode(torch.stack(batch[\"input_ids\"])[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([128000, 128000,    791,   4409,    315,   9853,  12167,   6011,    315,\n",
      "         21995,   8471,    374,   8647,    369,   8405,  10065,   2585,   3600,\n",
      "           311,    279,   4409,    315,   9853,  12167,     13,    578,   9476,\n",
      "           374,  11411,    311,  22923,    279,  23460,    315,  10099,    323,\n",
      "           279,   4029,    555,   8405,    264,   8205,    315,   3600,     11,\n",
      "          2737,  10065,   2585,     11,  25375,     11,    323,   6873,    627,\n",
      "           791,   9476,  27149,   1403,  10065,  52888,     11,    279,   4410,\n",
      "          9853,  12167,  21995,  75921,    323,    279,   6460,  13345,  21995,\n",
      "         75921,     11,    902,    527,   1825,    311,    279,    586,    369,\n",
      "         10065,   1008,   2945,     11,   5675,    323,   1766,   3600,     11,\n",
      "           323,   1023,  10065,  14228,   7640,     13,    578,   9476,   1101,\n",
      "           706,    264,   2128,    315,  10065,   2585,   9808,    889,   6013,\n",
      "           311,   6880,    369,   2532,    323,   3493,  13291,    449,  10065,\n",
      "         14228,   4819,    627,    644,   5369,    311,   1202,  23756,    323,\n",
      "         10065,   2585,   3600,     11,    279,   9476,   6209,    264,   8205,\n",
      "           315,   7620,    323,   3600,     11,   2737,    512,  42515,  25375,\n",
      "            25,    578,   9476,    706,    264,   7029,   2134,    315,  10099,\n",
      "          2561,    369,  25375,     11,   2737,  12875,     11,  19987,     11,\n",
      "         70244,     11,    323,   1023,   2678,  56669,    627,  48353,    323,\n",
      "          1766,     25,    578,   9476,   5825,    264,   5675,    323,   1766,\n",
      "          2532,    369,  10099,    430,    527,   5675,    477,   1766,    304,\n",
      "           279,   4409,    315,   9853,  12167,    627,   6540,    352,  90410,\n",
      "         29051,     25,    578,   9476,   6209,    993,    352,  90410,  29051,\n",
      "          3600,    369,  10099,     11,    902,   8779,    311,   8108,    279,\n",
      "          1396,    315,  36021,    326,  29163,    323,   5471,  10065,    927,\n",
      "         45541,    627,     53,   4575,   2617,     25,    578,   9476,   5825,\n",
      "         47165,   3600,    369,  10099,     11,    902,   8779,    311,   6144,\n",
      "          1124,    505,  19338,    627,  40234,     25,    578,   9476,   6209,\n",
      "          6873,    323,  47210,   7620,    311,  12192], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "response, output = generate_response(\n",
    "    candidate_model,\n",
    "    candidate_tokenizer,\n",
    "    batch[\"input_ids\"],\n",
    "    batch[\"attention_mask\"],\n",
    "    candidate_generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([258])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|>The City of Los Angeles Department of Animal Services is responsible for providing animal control services to the City of Los Angeles. The department is committed to promoting the welfare of animals and the community by providing a variety of services, including animal control, adoption, and education.\n",
      "The department operates two animal shelters, the West Los Angeles Animal Shelter and the East Valley Animal Shelter, which are open to the public for animal adoptions, lost and found services, and other animal-related activities. The department also has a team of animal control officers who respond to calls for service and provide assistance with animal-related issues.\n",
      "In addition to its shelter and animal control services, the department offers a variety of programs and services, including:\n",
      "Animal adoption: The department has a wide range of animals available for adoption, including dogs, cats, rabbits, and other small mammals.\n",
      "Lost and found: The department provides a lost and found service for animals that are lost or found in the City of Los Angeles.\n",
      "Spay/neuter: The department offers spay/neuter services for animals, which helps to reduce the number of unwanted litters and prevent animal overpopulation.\n",
      "Vaccination: The department provides vaccination services for animals, which helps to protect them from diseases.\n",
      "Education: The department offers education and outreach programs to promote\n"
     ]
    }
   ],
   "source": [
    "print(candidate_tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enshiallah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"out/results_240623023136_240628153415.json\", \"r\") as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "table = wandb.Table(columns=list(results[0].keys()))\n",
    "for r in results:\n",
    "    table.add_data(*r.values())\n",
    "run =wandb.init(project=\"huggingface\", entity=\"my-ku-org\", name=\"laaj-llama-3-8b-medical-v240623023136\")\n",
    "wandb.log({\"Evaluation Results\": table})\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
