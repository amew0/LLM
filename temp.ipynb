{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m vllm.entrypoints.openai.api_server --host 127.0.0.1 --port 8881 --model amew0/Meta-Llama-3-8B-Instruct-v240714045919 --dtype float16 --download-dir /dpc/kunf0097/l3-8b/model & ../ngrok http 8881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnas = \" \\n\\n### 1\\nQ: What is the patient's liver condition based on the provided lab results?\\nM: cirrhosis, chronic hepatitis, liver damage, liver failure\\nA: chronic hepatitis\\n\\n### 2\\nQ: What is the patient's serum albumin level?\\nM: 3.4, 7.54, 11.3, 15.6\\nA: 3.4\\n\\n### 3\\nQ: Which of the following is a recommendation for the patient's treatment?\\nM: avoid fatty diet, take beta blocker, take more sugar cane juice, consult gastroenterologist\\nA: consult gastroenterologist\\n\\n### 4\\nQ: What is the patient's bilirubin level?\\nM: 2.38, 4.88, 17.16, 25.8\\nA: 17.16\\n\\n### 5\\nQ: What is the patient's creatinine level?\\nM: 2.5, 4.88, 7.5, 10.5\\nA: 4.88\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = qnas.find(\"### 1\")\n",
    "\n",
    "qna_section = qnas[start_index:].strip()\n",
    "\n",
    "# Skip the empty string before the first ###\n",
    "qna_blocks = qna_section.split(\"### \")[1:]\n",
    "\n",
    "\n",
    "qnas_formatted = []\n",
    "for block in qna_blocks:\n",
    "    lines = block.strip()\n",
    "    q = lines[lines.find(\"Q: \"):lines.find(\"A: \")].strip()\n",
    "    a = lines[lines.find(\"A: \"):].strip()\n",
    "    \n",
    "    qnas_formatted.append({\"Q\": q, \"A\":a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnas_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "from scipy.stats import pearsonr\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from time import time\n",
    "import gc\n",
    "import json\n",
    "import yaml\n",
    "import argparse\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import fire\n",
    "import inspect\n",
    "\n",
    "\n",
    "logg = lambda x: print(f\"------------------------ {x} ---------------------------\")\n",
    "\n",
    "\n",
    "def inspectt(frame):\n",
    "    logg(\"\")\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    for arg in args:\n",
    "        print(f\"\\t{arg}: {values[arg]}\")\n",
    "    logg(\"\")\n",
    "\n",
    "def get_prompts_from_template(filepath, name, eval_name):\n",
    "    default_config = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "\n",
    "    candidate_prompt = data[name][\"candidate_prompt\"]\n",
    "    evaluator_prompt = data[eval_name][\"evaluator_prompt\"]\n",
    "    candidate_generation_config = data[name].get(\"candidate_generation_config\", default_config)\n",
    "    evaluator_generation_config = data[eval_name].get(\n",
    "        \"evaluator_generation_config\", default_config\n",
    "    )\n",
    "\n",
    "    print(\"candidate_prompt: \", candidate_prompt)\n",
    "    print(\"evaluator_prompt: \", evaluator_prompt)\n",
    "    print(\"candidate_generation_config: \", candidate_generation_config)\n",
    "    print(\"evaluator_generation_config: \", evaluator_generation_config)\n",
    "\n",
    "    return (\n",
    "        candidate_prompt,\n",
    "        evaluator_prompt,\n",
    "        candidate_generation_config,\n",
    "        evaluator_generation_config,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_tokenizer_and_model(model_name: str, cache_dir: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=f\"{cache_dir}/tokenizer\",\n",
    "        pad_token_id=0,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=f\"{cache_dir}/model\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        offload_buffers=True,\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def tokenize(prompt, tokenizer):\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(batch, tokenizer, prompt_template):\n",
    "    # print(batch)\n",
    "    prompts = [prompt_template.format(d[0], d[1]) for d in zip(batch[\"instruction\"], batch[\"input\"])]\n",
    "    print(\"a\", prompts)\n",
    "    tokenized_prompts = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "    print(\"b\", tokenized_prompts)\n",
    "    return tokenized_prompts\n",
    "\n",
    "\n",
    "def eval_prompt_tokenizer(generated, output, eval_tokenizer, prompt=None):\n",
    "    prompt = prompt.format(generated, output)\n",
    "    tokenized_full_prompt = tokenize(prompt, tokenizer=eval_tokenizer)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "\n",
    "def extract_score(text):\n",
    "    match = re.search(r\"\\b\\d+\\.\\d+\\b\", text)\n",
    "    return float(match.group(0)) if match else -1.0\n",
    "\n",
    "\n",
    "def log2json(results, json_result):\n",
    "    with open(json_result, \"w\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def generate_response(model, tokenizer, input_ids, attention_mask, generation_config):\n",
    "    # torch.LongTensor(input_ids).to(model.device)\n",
    "    # torch.LongTensor(attention_mask).to(model.device)\n",
    "    # try:\n",
    "        output = model.generate(\n",
    "            input_ids=torch.stack(input_ids).to(model.device),\n",
    "            attention_mask=torch.stack(attention_mask).to(model.device),\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            **generation_config,\n",
    "        )\n",
    "        print(output[0])\n",
    "        response_ids = output[0][len(input_ids[0]) :]\n",
    "        response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "        return response, output\n",
    "    # except RuntimeError as e:\n",
    "    #     if \"inf\" in str(e) or \"nan\" in str(e):\n",
    "    #         print(f\"Skipping example due to invalid output: {e}\")\n",
    "    #         return None\n",
    "    #     else:\n",
    "    #         raise \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=f\"./out\"\n",
    "cache_dir=f\"/dpc/kunf0097/l3-8b\"\n",
    "eval_data_path=\"./data/1/eval_sample.json\"\n",
    "log_file=None\n",
    "name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "eval_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "run_id=datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "log2wandb: bool = True\n",
    "project=\"huggingface\"\n",
    "entity=\"my-ku-org\"\n",
    "evals_per_example=2\n",
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "evaluator_name=\"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    candidate_prompt,\n",
    "    evaluator_prompt,\n",
    "    candidate_generation_config,\n",
    "    evaluator_generation_config,\n",
    ") = get_prompts_from_template(\"template.yaml\", candidate_name, evaluator_name)\n",
    "\n",
    "\n",
    "if log2wandb and (project is None or entity is None):\n",
    "    raise ValueError(\"Both 'project' and 'entity' must be set if 'log2wandb' is True.\")\n",
    "\n",
    "if log_file is None:\n",
    "    log_file = f\"{output_dir}/results_{name.split('/')[1]}_{run_id}.json\"\n",
    "\n",
    "inspectt(inspect.currentframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator_tokenizer, evaluator_model = get_tokenizer_and_model(\n",
    "#     model_name=eval_name, cache_dir=cache_dir\n",
    "# )\n",
    "\n",
    "candidate_tokenizer, candidate_model = get_tokenizer_and_model(\n",
    "    model_name=name, cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# candidate_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#         name,\n",
    "#         cache_dir=f\"{cache_dir}/tokenizer\",\n",
    "#         pad_token_id=0,\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_tokenizer.pad_token = candidate_tokenizer.bos_token \n",
    "candidate_tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=eval_data_path)\n",
    "eval_dataset = data[\"train\"].map(\n",
    "    lambda x: generate_and_tokenize_prompt(x, candidate_tokenizer, candidate_prompt),\n",
    "    batched=True,  # Process in batches\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_eval_dataset  =torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batched_eval_dataset :\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(batch[\"input_ids\"])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(batch[\"attention_mask\"])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidate_tokenizer.decode(torch.stack(batch[\"input_ids\"])[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, output = generate_response(\n",
    "    candidate_model,\n",
    "    candidate_tokenizer,\n",
    "    batch[\"input_ids\"],\n",
    "    batch[\"attention_mask\"],\n",
    "    candidate_generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidate_tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enshiallah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"out/results_240623023136_240628153415.json\", \"r\") as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "table = wandb.Table(columns=list(results[0].keys()))\n",
    "for r in results:\n",
    "    table.add_data(*r.values())\n",
    "run =wandb.init(project=\"huggingface\", entity=\"my-ku-org\", name=\"laaj-llama-3-8b-medical-v240623023136\")\n",
    "wandb.log({\"Evaluation Results\": table})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evals **MMLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =load_dataset(\"cais/mmlu\", \"clinical_knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = data[\"dev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu-11-2\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import socket\n",
    "print(socket.gethostname())\n",
    "\n",
    "# %%\n",
    "import gc\n",
    "import inspect\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import fire\n",
    "import huggingface_hub\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from utils.eval_helper import inspectt, logg\n",
    "from utils.ft_helper import (\n",
    "    generate_and_tokenize_prompt,\n",
    "    get_start_index,\n",
    "    reorder_dataset,\n",
    ")\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "wandb.require(\"core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/kunet.ae/ku5001069/.cache/huggingface/token\n",
      "Login successful\n",
      "------------------------  ---------------------------\n",
      "------------------------  ---------------------------\n",
      "{'prompt': '### System:\\n{}<|endoftext|>\\n### User:\\n{}<|endoftext|>\\n### Assistant:\\n', 'response': '{}<|endoftext|>\\n', 'peft_args': {'r': 4, 'lora_alpha': 16, 'target_modules': ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value'], 'lora_dropout': 0.05, 'bias': 'none', 'task_type': 'CAUSAL_LM'}, 'training_args': {'warmup_ratio': 0.1, 'num_train_epochs': 3, 'learning_rate': 0.0003, 'fp16': False, 'logging_steps': 1, 'optim': 'adamw_torch', 'group_by_length': False, 'dataloader_drop_last': False, 'save_steps': 2, 'save_total_limit': 3}}\n"
     ]
    }
   ],
   "source": [
    "cache_dir: str = f\"/dpc/kunf0097/l3-8b\"\n",
    "train_data_path: str = \"./data/medical-36-row.json\"\n",
    "model_name: str = \"EleutherAI/pythia-70m-deduped\"\n",
    "model_save_path: str = None\n",
    "run_id: str = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "chpt_dir: str = None\n",
    "last_checkpoint: str = None\n",
    "start_index: int = 0\n",
    "cutoff_len: int = 298\n",
    "per_device_train_batch_size: int = 1\n",
    "gradient_accumulation_steps: int = 1\n",
    "world_size: int = None\n",
    "local_rank: int = None\n",
    "\n",
    "load_dotenv()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "HF_TOKEN_WRITE = os.getenv(\"HF_TOKEN_WRITE\")\n",
    "if HF_TOKEN_WRITE is not None:\n",
    "    huggingface_hub.login(token=HF_TOKEN_WRITE)\n",
    "\n",
    "if model_save_path is None:\n",
    "    model_save_path = f\"{cache_dir}/model/{model_name}-v{run_id}\"\n",
    "\n",
    "if chpt_dir is None:\n",
    "    chpt_dir = f\"{cache_dir}/chpt/{run_id}\"\n",
    "\n",
    "if os.path.isdir(chpt_dir):\n",
    "    checkpoints = [d for d in os.listdir(chpt_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        last_checkpoint = os.path.join(\n",
    "            chpt_dir, max(checkpoints, key=lambda cp: int(cp.split(\"-\")[-1]))\n",
    "        )\n",
    "\n",
    "# if train_data_path locally exists use it\n",
    "if os.path.exists(train_data_path):\n",
    "    data = load_dataset(\"json\", data_files=train_data_path, split=\"train\")\n",
    "else:\n",
    "    data = load_dataset(train_data_path, split=\"train\")\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    start_index = get_start_index(last_checkpoint, len(data))\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": local_rank}\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "inspectt(inspect.currentframe())\n",
    "\n",
    "with open(f\"tuning.yaml\", \"r\") as f:\n",
    "    ft_config = yaml.safe_load(f)[model_name]\n",
    "    assert \"training_args\" in ft_config, \"Training arguments are not defined in tuning.yaml\"\n",
    "    assert \"peft_args\" in ft_config, \"Peft arguments are not defined in tuning.yaml\"\n",
    "    print(ft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=f\"{cache_dir}/tokenizer\")\n",
    "\n",
    "# Initialize model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "\n",
    "\n",
    "    # 8bit\n",
    "\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=f\"{cache_dir}/model\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer has no pad token. Adding it.\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    print(\"Tokenizer has no pad token. Adding it.\")\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if start_index != 0:\n",
    "    data = reorder_dataset(data, start_index)\n",
    "train_dataset = data.map(lambda x: generate_and_tokenize_prompt(x, tokenizer, ft_config, cutoff_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Conv1D\n",
    "\n",
    "def get_specific_layer_names(model):\n",
    "    # Create a list to store the layer names\n",
    "    layer_names = []\n",
    "    \n",
    "    # Recursively visit all modules and submodules\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is an instance of the specified layers\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n",
    "            # model name parsing \n",
    "            print(name)\n",
    "\n",
    "            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n",
    "    \n",
    "    return layer_names\n",
    "\n",
    "list(set(get_specific_layer_names(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LoRA\n",
    "peft_args = ft_config[\"peft_args\"]\n",
    "peft_config = LoraConfig(**peft_args)\n",
    "\n",
    "\n",
    "training_args = ft_config[\"training_args\"]\n",
    "train_config = SFTConfig(\n",
    "    run_name=f\"ft-{model_name.split('/')[1]}-{run_id}-v{start_index}\",\n",
    "    resume_from_checkpoint=last_checkpoint,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    output_dir=f\"{chpt_dir}\",\n",
    "    max_seq_length=cutoff_len, # not sure its purpose since its setup on the tokenizer\n",
    "    eval_accumulation_steps=1,  # !very important to send data to cpu\n",
    "    **training_args\n",
    ")\n",
    "\n",
    "class SFTTrainerNoShuffle(SFTTrainer):\n",
    "    def training_step(self, model, inputs):\n",
    "        if (self.state.global_step % self.args.save_steps) == 0:\n",
    "            inputs_decoded = tokenizer.decode(inputs[\"input_ids\"][0])\n",
    "            logger.critical(f\"{self.state.global_step}: {inputs_decoded}\")\n",
    "        return super().training_step(model, inputs)\n",
    "\n",
    "    def _get_train_sampler(self):\n",
    "        return SequentialSampler(self.train_dataset)  # to prevent shuffling\n",
    "\n",
    "    # [Not saving to trainer_state]\n",
    "    def save_state(self):\n",
    "        self.state.gradient_accumulation_steps = self.args.gradient_accumulation_steps\n",
    "        super().save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainerNoShuffle(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    args=train_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamew0\u001b[0m (\u001b[33mmy-ku-org\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kunet.ae/ku5001069/LLM/wandb/run-20240730_152147-14g5pahb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/my-ku-org/huggingface/runs/14g5pahb' target=\"_blank\">ft-pythia-70m-deduped-240730152135-v0</a></strong> to <a href='https://wandb.ai/my-ku-org/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/my-ku-org/huggingface' target=\"_blank\">https://wandb.ai/my-ku-org/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/my-ku-org/huggingface/runs/14g5pahb' target=\"_blank\">https://wandb.ai/my-ku-org/huggingface/runs/14g5pahb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.. Thank you doc!<|endoftext|>\n",
      "### Assistant:\n",
      "Hi, Thank you for posting your query. The most likely cause for your symptoms is benign paroxysmal positional vertigo (BPPV), a type of peripheral vertigo. In this condition, the most common symptom is dizziness or giddiness, which is made worse with movements. Accompanying nausea and vomiting are common. The condition is due to problem in the ear, and improves in a few days on own. Betahistine tablets would help relieve your symptoms. Doing vestibular rehabilitation or adaptation exercises would prevent the recurrence of these symptoms. An ENT evaluation would also help. I hope it helps. Best\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>15.664400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>24.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12.924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>9.864600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>25.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>14.498300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>14.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9.749700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>14.884600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>15.329200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>23.940300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>12.614700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>9.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>24.923100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>14.257900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>14.243700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>9.561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>14.674500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>15.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>23.594300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>12.472300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>9.443300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>24.708300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>14.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>14.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>9.487300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "2: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "hi my nine year old son had a cough and flu symptons three months ago and the chesty sounding cough and green phlegm still remains. it did seem to get better but never totally went and has now picked up again...he has only ever had antibiotics once in his live which suggests he his generally fitand well and active...never short of breath or weezy...so why would this be....<|endoftext|>\n",
      "### Assistant:\n",
      "Hi, If the symptoms persist that long this suggests the presence of an allergic element. To treat that kind of allergy you can give him an over the counter antihistamines once daily before going to bed. A cough suppressant as dextromethorphan and an expectorant as Murine will help reduce the cough. Make sure he Chat Doctor.  If the green phlegm persists he might require an antibiotic. Hope I have answered your query. Let me know if I can assist you further.<|endoftext|>\n",
      "[PAD][PAD][PAD][PAD]\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "4: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "I had a alt reading about 2+ months ago of 43 then retested today and got a 60 alt reading my other readings came back normal should I be concerned?Im 40 years 5ft 3 and weigh 192albs have a history of chronic vertigo and insomnia, and chronic sinus problems. I take valium 5 mg 4 times a day, fluticansone 2 puffs each nose once a day, qvair two puffs twice a day to prevent weasing, claritin for allergies, and I do sinus rinses as needed. I am female.My doctor says none of the medicine has liver side effects, and I dont drink alcohol but a sip on the rare occasion. He thinks it could be do my weight and Im working on this with gradual exercise when Im able too. Should I be worried with this test result? I had one other result that was high my triglcyerides read 198? WWW.WWWW.WW<|endoftext|>\n",
      "### Assistant:\n",
      "Hello and thank you for asking Chat Doctor. I have read your report, and I understand your concerns. First I would say to not worry. Normal Alt levels are 7 to 56 unit (depending on from the kits that laboratories use)In one of your tests, your Alt was 60. This level is almost normal and it doesn't always mean\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "6: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "Hi hope you can helpive been getting chest pains, not severe but a constant pain, can happen when im just resting but has also happened when im exercising, it continues through to my back as well. I sometimes wake up with chest pains too. i have a strong history of heart disease. grandfather died at 26 from a heat attack and mother had one at 45, also grandmother on the other side had a heart attack. Could you please explain what is happening? Thanks<|endoftext|>\n",
      "### Assistant:\n",
      "HelloThanks for posting at Chat Doctor. You have not provided your age on the details. I would have been able to help u better had you mentioned your age. You have complaints of chest pain radiating to back with strong family history. So in your case it is better that we evaluate you for presence of heart disease. I recommend an ECG and 2d echo initially. If both these tests are normal you can proceed for a treadmill test which requires you to walk on a treadmill while your ECG is continuously monitored. This will show if the heart blood supply suffers while you are exercising which indirectly predicts whether you have blockages in your heart. Please do the above tests and revert. Wishing you good healthRegards.<|endoftext|>\n",
      "\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "8: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "I am on metformin 1000mg bid for pcos & was prescribed Percocet 10/650 for a severely displaced fracture needing surgery. I drink a glass of red wine nightly for heart heglth Bhc cardiac disease runs in my family... Should I be concerned a our elastase liver function test? I am also on Zoloft 150 daily, adderall 30mg bid & clonazepam as needed who I take a few night a a week<|endoftext|>\n",
      "### Assistant:\n",
      "Hi, dairy have gone through your question. I can understand your concern. You taken percent, clonazepam, Zoloft, Adderall so in my opinion you should avoid alcohol. It will damage the liver and will put you in trouble. You should check your liver enzymes levels once and also go for ultrasound abdomen once to rule out any alcoholic liver disease. Hope I have answered your question. If you have any doubts then feel free to ask me. I will be happy to answer. Thanks for using Chat Doctor. Wish you a very good health.<|endoftext|>\n",
      "[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "10: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "I have a 5 month old baby who is very congested with a terrible cough. Its rattly/raspy and croupy sounding cough. She started choking on her coughs and the mucous that has come up. She also has a fever and runny nose. Should i take her to urgent care?<|endoftext|>\n",
      "### Assistant:\n",
      "Thank you for using Chat Doctor. I would suggest that you see your doctor. Your baby maybe having bronchiolitis which is a lung infection common to your kids age. It is commonly caused by a virus. Albuterol via nebulization should be utilized in order to alleviate the wheezing and also help with the congestion. A decongestant can also be used for the colds. Also, it would also advise doing a chest X-ray in order to rule out other diseases (ex. pneumonia)sincerely, Mark RosarioGeneral pediatrics/Pediatric Pulmonology<|endoftext|>\n",
      "[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "12: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "hye, My aunt is having shortness of breath and she is on vent now. Her breast showed some kind of infection that apparently turned out to have black blisters. I could she those spreading. When we went for lungs x-ray doctor said she has accumulation of water in her lungs<|endoftext|>\n",
      "### Assistant:\n",
      "Thanks for your question on Chat Doctor. I can understand your aunts situation and problem. By your history and description, possibility of bacterial infection especially staphylococcus is more in her case. She is having pleural effusion and infective skin lesions on breast. Staphylococcus can cause pleural effusion and blister formation on skin. So chances of staphylococcal infection is more in her case. Better to send pleural fluid culture and sensitivity for the diagnosis of staphylococcal infection. This will also tell about effective antibiotic therapy. With appropriate antibiotics and Care, this infection can be treated. Hope I have solved your query. Wishing good health to your aunt. Thanks.<|endoftext|>\n",
      "[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "14: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "for the pst  6 days  i have been haing  upper  abdome  distress...  i have had gall bladder  surgery  2 yrs  ago..in the past  2  yrs  when i get these bouts .. i go to  emerg  they run cardiograms  xrays,blood work,all comes back normal ...im stressing out  over this  as im  scared every time it happens  im having a heart  attack  wht is it ?<|endoftext|>\n",
      "### Assistant:\n",
      "Hi. Thanks for your query, read and understood your problems. You are getting bouts of pain in the upper abdomen within the last 2 years particularly after the gall bladder surgery. This is great news that cardiac problems are ruled out by the tests every time in the emergency Room. This means that we have to find a local cause in the abdomen itself. I would suggest the following<|endoftext|>\n",
      "[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "16: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "my mother is 80 yrs. detected with vulva cancer. vulva is swollen red,around discolourisation, white growth now extending to rectum. biopsy shows keratinizing squamous carcinoma vulva(welldifferentiated,invasive).she is little asthmatic, has cervical spondeolytis.<|endoftext|>\n",
      "### Assistant:\n",
      "Hi, dairy have gone through your question. I can understand your concern. She has well differentiated keratinizing squamous cell carcinoma.  If her general health is good then treatment of choice is wide excision of carcinoma followed by chemotherapy or radiotherapy if needed. Consult your doctor and take treatment accordingly. Hope I have answered your question, if you have doubt then I will be happy to answer. Thanks for using Chat Doctor. Wish you a very good health.<|endoftext|>\n",
      "[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "18: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bathroom walking unsteadily, as i tried to focus i feel nauseous. I try to vomit but it wont come out.. After taking panadol and sleep for few hours, i still feel the same.. By the way, if i lay down or sit down, my head do not spin, only when i want to move around then i feel the whole world is spinning.. And it is normal stomach discomfort at the same time? Earlier after i relieved myself, the spinning lessen so i am not sure whether its connected or coincidences.. Thank you doc!<|endoftext|>\n",
      "### Assistant:\n",
      "Hi, Thank you for posting your query. The most likely cause for your symptoms is benign paroxysmal positional vertigo (BPPV), a type of peripheral vertigo. In this condition, the most common symptom is dizziness or giddiness, which is made worse with movements. Accompanying nausea and vomiting are common. The condition is due to problem in the ear, and improves in a few days on own. Betahistine tablets would help relieve your symptoms. Doing vestibular rehabilitation or adaptation exercises would prevent the recurrence of these symptoms. An ENT evaluation would also help. I hope it helps. Best\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "20: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "hi my nine year old son had a cough and flu symptons three months ago and the chesty sounding cough and green phlegm still remains. it did seem to get better but never totally went and has now picked up again...he has only ever had antibiotics once in his live which suggests he his generally fitand well and active...never short of breath or weezy...so why would this be....<|endoftext|>\n",
      "### Assistant:\n",
      "Hi, If the symptoms persist that long this suggests the presence of an allergic element. To treat that kind of allergy you can give him an over the counter antihistamines once daily before going to bed. A cough suppressant as dextromethorphan and an expectorant as Murine will help reduce the cough. Make sure he Chat Doctor.  If the green phlegm persists he might require an antibiotic. Hope I have answered your query. Let me know if I can assist you further.<|endoftext|>\n",
      "[PAD][PAD][PAD][PAD]\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "22: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "I had a alt reading about 2+ months ago of 43 then retested today and got a 60 alt reading my other readings came back normal should I be concerned?Im 40 years 5ft 3 and weigh 192albs have a history of chronic vertigo and insomnia, and chronic sinus problems. I take valium 5 mg 4 times a day, fluticansone 2 puffs each nose once a day, qvair two puffs twice a day to prevent weasing, claritin for allergies, and I do sinus rinses as needed. I am female.My doctor says none of the medicine has liver side effects, and I dont drink alcohol but a sip on the rare occasion. He thinks it could be do my weight and Im working on this with gradual exercise when Im able too. Should I be worried with this test result? I had one other result that was high my triglcyerides read 198? WWW.WWWW.WW<|endoftext|>\n",
      "### Assistant:\n",
      "Hello and thank you for asking Chat Doctor. I have read your report, and I understand your concerns. First I would say to not worry. Normal Alt levels are 7 to 56 unit (depending on from the kits that laboratories use)In one of your tests, your Alt was 60. This level is almost normal and it doesn't always mean\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "24: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "Hi hope you can helpive been getting chest pains, not severe but a constant pain, can happen when im just resting but has also happened when im exercising, it continues through to my back as well. I sometimes wake up with chest pains too. i have a strong history of heart disease. grandfather died at 26 from a heat attack and mother had one at 45, also grandmother on the other side had a heart attack. Could you please explain what is happening? Thanks<|endoftext|>\n",
      "### Assistant:\n",
      "HelloThanks for posting at Chat Doctor. You have not provided your age on the details. I would have been able to help u better had you mentioned your age. You have complaints of chest pain radiating to back with strong family history. So in your case it is better that we evaluate you for presence of heart disease. I recommend an ECG and 2d echo initially. If both these tests are normal you can proceed for a treadmill test which requires you to walk on a treadmill while your ECG is continuously monitored. This will show if the heart blood supply suffers while you are exercising which indirectly predicts whether you have blockages in your heart. Please do the above tests and revert. Wishing you good healthRegards.<|endoftext|>\n",
      "\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "26: ### System:\n",
      "If you are a doctor, please answer the medical questions based on the patient's description.<|endoftext|>\n",
      "### User:\n",
      "I am on metformin 1000mg bid for pcos & was prescribed Percocet 10/650 for a severely displaced fracture needing surgery. I drink a glass of red wine nightly for heart heglth Bhc cardiac disease runs in my family... Should I be concerned a our elastase liver function test? I am also on Zoloft 150 daily, adderall 30mg bid & clonazepam as needed who I take a few night a a week<|endoftext|>\n",
      "### Assistant:\n",
      "Hi, dairy have gone through your question. I can understand your concern. You taken percent, clonazepam, Zoloft, Adderall so in my opinion you should avoid alcohol. It will damage the liver and will put you in trouble. You should check your liver enzymes levels once and also go for ultrasound abdomen once to rule out any alcoholic liver disease. Hope I have answered your question. If you have any doubts then feel free to ask me. I will be happy to answer. Thanks for using Chat Doctor. Wish you a very good health.<|endoftext|>\n",
      "[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Elapsed time: 45.768951177597046 ---------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "\n",
    "start = time()\n",
    "if last_checkpoint is not None:\n",
    "    logg(\"Resuming from checkpoint\")\n",
    "    trainer.train(last_checkpoint)\n",
    "else:\n",
    "    trainer.train()\n",
    "end = time()\n",
    "logg(f\"Elapsed time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval / Chpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import inspect\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import fire\n",
    "import huggingface_hub\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from utils.eval_helper import inspectt, logg\n",
    "from utils.ft_helper import (\n",
    "    generate_and_tokenize_prompt,\n",
    "    get_start_index,\n",
    "    reorder_dataset,\n",
    ")\n",
    "from torch.utils.data import SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir: str = f\"/dpc/kunf0097/l3-8b\"\n",
    "train_data_path: str = \"meher146/medical_llama3_instruct_dataset\"\n",
    "model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_save_path: str = None\n",
    "run_id: str = \"240724111548\"\n",
    "chpt_dir: str = None\n",
    "last_checkpoint: str = None\n",
    "start_index: int = 0\n",
    "per_device_train_batch_size: int = 4\n",
    "gradient_accumulation_steps: int = 4\n",
    "world_size: int = None\n",
    "local_rank: int = None\n",
    "\n",
    "if model_save_path is None:\n",
    "    model_save_path = f\"{cache_dir}/model/{model_name}-v{run_id}\"\n",
    "\n",
    "if chpt_dir is None:\n",
    "    chpt_dir = f\"{cache_dir}/chpt/{run_id}\"\n",
    "\n",
    "if os.path.isdir(chpt_dir):\n",
    "    checkpoints = [d for d in os.listdir(chpt_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        last_checkpoint = os.path.join(\n",
    "            chpt_dir, max(checkpoints, key=lambda cp: int(cp.split(\"-\")[-1]))\n",
    "        )\n",
    "\n",
    "# if train_data_path locally exists use it\n",
    "if os.path.exists(train_data_path):\n",
    "    data = load_dataset(\"json\", data_files=train_data_path, split=\"train\")\n",
    "else:\n",
    "    data = load_dataset(train_data_path, split=\"train\")\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    start_index = get_start_index(last_checkpoint, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=f\"{cache_dir}/model\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(last_checkpoint, cache_dir=f\"{cache_dir}/tokenizer\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftmodel = PeftModel.from_pretrained(model, last_checkpoint)\n",
    "# ftmodel = ftmodel.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_TOKEN_WRITE = os.environ[\"HF_TOKEN_WRITE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftmodel.push_to_hub(f\"{model_name.split('/')[1]}-v{run_id}_si{start_index}-ada\", token=HF_TOKEN_WRITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.push_to_hub(f\"{model_name.split('/')[1]}-v{run_id}_si{start_index}\", token=HF_TOKEN_WRITE)\n",
    "ftmodel.push_to_hub(f\"{model_name.split('/')[1]}-v{run_id}_si{start_index}\", token=HF_TOKEN_WRITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"amew0/Meta-Llama-3-8B-Instruct-v240724111548_si110870\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     cache_dir=f\"{cache_dir}/model\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\", cache_dir=f\"{cache_dir}/tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./data/medical-1-row.json\"\n",
    "data = load_dataset(\"json\", data_files=train_data_path, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tokenize(prompt, tokenizer):\n",
    "    tokenized = tokenizer(prompt, return_tensors='pt')\n",
    "    # tokenized[\"input_ids\"] = tokenized[\"input_ids\"].flatten()\n",
    "    # tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"].flatten()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def eval_generate_and_tokenize_prompt(data_point, tokenizer):\n",
    "    # assistant_template = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    # i = data_point[\"prompt\"].find(assistant_template)\n",
    "    # user_prompt = data_point[\"prompt\"][:i+len(assistant_template)]\n",
    "    # print(data_point[\"prompt\"])\n",
    "    tokenized_full_prompt = eval_tokenize(data_point[\"prompt\"], tokenizer=tokenizer)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "eval_dataset = data.map(\n",
    "    lambda x: eval_generate_and_tokenize_prompt(x, tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = data[0]\n",
    "example = eval_generate_and_tokenize_prompt(example, tokenizer, prompt=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(example[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model.generate(\n",
    "    input_ids=example[\"input_ids\"].to(model.device),\n",
    "    attention_mask=example[\"attention_mask\"].to(model.device),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_helper import generate_response\n",
    "\n",
    "response = generate_response(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    example[\"input_ids\"],\n",
    "    example[\"attention_mask\"],\n",
    "    {},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_TOKEN_WRITE = os.environ[\"HF_TOKEN_WRITE\"]\n",
    "tokenizer.push_to_hub(f\"{model_name.split('/')[1]}-v{run_id}_si{start_index}\", token=HF_TOKEN_WRITE)\n",
    "ftmodel.push_to_hub(f\"{model_name.split('/')[1]}-v{run_id}_si{start_index}\", token=HF_TOKEN_WRITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "cache_dir: str = f\"/dpc/kunf0097/l3-8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m-deduped\", cache_dir=f\"{cache_dir}/tokenizer\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14e579306570>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache_dir = \"/dpc/kunf0097/l3-8b\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", cache_dir=f\"{cache_dir}/tokenizer\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\", cache_dir=f\"{cache_dir}/model\")\n",
    "config = AutoConfig.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Input Embeddings\n",
    "# import transformers\n",
    "torch.manual_seed(26)\n",
    "input_text = \"Hello, how are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"] # shape [1,6]\n",
    "embeddings = model.transformer.wte(input_ids) # shape [1,6,768] # wte = word token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([[1,2,3],[4,5,6]]).split(2,dim=1)\n",
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2],\n",
       "         [4, 5]]),\n",
       " tensor([[3],\n",
       "         [6]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 384])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = hidden_states.split(384, dim=2)\n",
    "# a.shape\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wz torch\n",
    "wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "input_embeds = wte(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Positional Encoding\n",
    "# Transformers are unaware of the order of tokens inherently, so they add positional encodings to embeddings.\n",
    "\n",
    "position_ids = torch.arange(input_ids.size(-1), dtype=torch.long).unsqueeze(0) # [0,1,2,3,4,5] # typical range i.e position\n",
    "positional_encodings = model.transformer.wpe(position_ids) # word positional encodings # shape [1,6,768] # postion_ids allowed are [0,1023]\n",
    "# encoded_input = embeddings + positional_encodings # encoded_input has info about a tokens representation and position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with torch\n",
    "torch.manual_seed(26)\n",
    "position_ids = torch.arange(input_ids.size(-1), dtype=torch.long).unsqueeze(0) # [0,1,2,3,4,5] # typical range i.e position\n",
    "wpe = nn.Embedding(config.max_position_embeddings, config.n_embd)\n",
    "position_embeds = wpe(position_ids)\n",
    "\n",
    "hidden_states = input_embeds + position_embeds\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 2304])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(768,768*3)(hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.43.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT2Block\n",
    "# config.num_hidden_layers # 12\n",
    "# nn.LayerNorm\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.conda/envs/llm201/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/llm201/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm201/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(model.transformer.wpe(torch.tensor(-1)).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8821e-02, -1.9742e-01,  4.0267e-03,  1.1347e-02,  6.3824e-02,\n",
       "        -1.0501e-01,  3.6937e-02, -1.6803e-01, -4.9111e-02, -5.6461e-02,\n",
       "        -2.4560e-03,  1.3503e-02, -4.1711e-03,  1.5115e-02,  1.6595e-02,\n",
       "        -1.3808e-01, -6.3314e-03, -4.6150e-02,  2.6675e-02, -2.0417e-01,\n",
       "         1.3454e-02, -3.6267e-02,  1.9301e-02, -2.5931e-02,  8.0243e-03,\n",
       "         8.4712e-03, -1.9906e-02,  6.6802e-02,  7.1151e-03, -2.6618e-02,\n",
       "         2.0829e-02, -3.3732e-02, -8.2898e-03,  9.8622e-03, -2.7369e-02,\n",
       "        -9.9118e-02, -7.5254e-01,  2.3550e-02, -3.0513e-02,  7.7456e-02,\n",
       "         3.4301e-03,  7.1132e-03,  2.6479e-02, -1.2113e-03,  1.1219e-01,\n",
       "        -2.0606e-03, -2.2458e-02, -2.2287e-02,  2.3570e-02,  3.9777e-01,\n",
       "         1.8856e-02,  2.0280e-02,  6.3043e-01,  2.3146e-02, -4.6894e-02,\n",
       "         4.0653e+00, -1.7403e-02, -5.1683e-02,  7.2271e-02, -7.9312e-02,\n",
       "         4.0248e-02,  1.9908e-02, -4.6380e-02, -2.8380e-02,  7.2535e-03,\n",
       "         2.6772e-02,  1.4972e-03, -2.9892e-01, -1.1627e-01,  9.1860e-03,\n",
       "        -2.3379e-02,  7.8674e-02,  3.6326e-02, -4.1279e-02,  3.5415e-02,\n",
       "         2.6852e-02,  3.7368e-04,  4.5535e-03,  2.5672e-02, -3.8062e-02,\n",
       "         1.5198e-02, -1.1324e-03, -3.6556e-02, -9.6938e-03, -7.7945e-02,\n",
       "        -5.0139e-02, -1.0231e+00, -1.2559e-02, -1.8484e-03,  6.6347e-02,\n",
       "         1.1819e-03, -3.3319e-02, -1.1513e-01,  2.1627e-02, -4.3646e-02,\n",
       "        -1.1303e-02, -7.7890e-03,  1.6647e-02, -7.2352e-03,  1.0707e-02,\n",
       "         1.3787e-02, -3.7777e-02, -1.2361e-01, -1.7224e-01, -1.8045e-02,\n",
       "         6.3456e-03, -4.8349e-02,  4.1328e-01, -1.0673e-03, -1.1704e-02,\n",
       "         1.6934e-02,  3.2545e-03, -2.3711e-02,  5.9829e-02, -5.9910e-02,\n",
       "        -1.4971e-02, -8.3736e-01, -3.8427e-02, -7.6987e-03,  4.9164e-02,\n",
       "         2.3555e-02, -5.3212e-02,  1.8699e-03, -1.5544e-01,  6.5622e-02,\n",
       "         5.2014e-03, -1.3533e-02,  2.8135e-02, -1.0581e-01,  5.5449e-02,\n",
       "         3.4420e-02,  1.4542e-02,  1.7542e-02, -2.5723e-03, -1.6590e-03,\n",
       "         1.6827e-02, -3.0037e-02, -1.1984e-01,  8.1251e-02, -4.5297e-01,\n",
       "         1.6045e-02,  4.2054e-03,  3.4240e-02, -2.0139e-02, -3.2975e-02,\n",
       "        -5.1996e-02, -3.6392e-02, -1.1540e-02,  2.2939e-02,  7.3400e-02,\n",
       "        -4.6437e-02, -8.2206e-02, -1.3095e-02,  1.5196e-02,  1.6183e-01,\n",
       "        -5.1890e-02,  4.9133e-02,  5.4630e-03, -3.1565e-02, -2.2381e-02,\n",
       "        -1.6066e-01,  2.7186e-02,  3.3445e-02, -2.9198e-02,  3.6843e-02,\n",
       "         1.0586e-03,  2.1095e-02,  6.4639e-02, -2.7470e-02,  8.8352e-02,\n",
       "         5.5776e-02, -2.3872e-02, -1.2993e-02, -2.6942e-04,  1.0038e-01,\n",
       "        -9.2795e-02, -7.9185e-01, -4.2195e-02,  2.4330e-02, -1.9673e-02,\n",
       "        -2.2679e-02,  1.3850e-02, -2.0604e-02,  9.8258e-02,  3.8011e-02,\n",
       "         2.3985e-02,  2.6789e-02,  3.1317e-02, -1.0934e-02, -7.4405e-02,\n",
       "        -4.1879e-02, -7.8305e-03, -9.1414e-02,  1.4571e-01, -2.2372e-02,\n",
       "        -7.4952e-02,  3.6658e-02,  4.8322e-03,  1.1366e-02,  7.7893e-02,\n",
       "        -5.3751e-02, -1.1380e-02, -1.0407e-02, -1.5257e-02,  3.0442e-02,\n",
       "        -5.0440e-03,  2.6727e-02,  5.8141e-03, -3.8137e-02,  1.3994e-02,\n",
       "        -2.9547e-03,  4.7238e-02,  5.7311e-02,  6.8433e-02, -4.4144e-02,\n",
       "         4.0254e-03,  3.9776e-02, -3.2052e-02,  2.8832e-02, -2.5070e-02,\n",
       "        -3.2514e-03, -7.3319e-02,  5.0036e-02, -7.9418e-02, -1.7448e-02,\n",
       "        -6.5178e-03,  8.9161e-03,  1.6941e-02,  2.8391e-02,  1.0850e-02,\n",
       "         1.5500e-02, -1.5806e-02, -1.5287e-02, -2.9555e-02, -4.0786e-02,\n",
       "        -5.6020e-02, -3.3106e-02,  3.6305e-02, -5.9147e-03, -5.6259e-03,\n",
       "        -3.0364e-03,  1.1340e-01,  2.6560e-02,  1.6906e-03,  1.5023e-02,\n",
       "        -3.3559e-02,  1.6866e-02, -3.8061e-02,  7.5497e-03,  1.0009e-02,\n",
       "         2.3764e-02,  3.2676e-02, -8.4955e-03,  8.1304e-03, -6.6838e-02,\n",
       "         6.7926e-02, -3.7586e-02,  4.8547e-02,  8.3520e-02,  2.8259e-02,\n",
       "         6.7701e-03, -9.8969e-02,  1.6984e-02,  1.0438e-02, -4.1560e-02,\n",
       "         9.8130e-03, -3.7107e-03,  1.7649e-02, -2.9944e-02,  1.1517e-02,\n",
       "         2.1170e+00,  8.0320e-01, -2.7883e-02, -2.5582e-02, -2.4898e-02,\n",
       "         2.4827e-02, -3.8706e-01,  3.0922e-02,  5.3630e-02, -2.1325e-02,\n",
       "        -1.7633e-02,  1.4735e-02, -1.3573e-02,  9.3698e-02,  6.3691e-02,\n",
       "        -8.5184e-03, -3.6612e-02,  1.6177e-02, -4.6694e-01,  6.3364e-01,\n",
       "        -6.5773e-02,  5.9070e-02,  4.0830e-02,  3.7846e-02,  2.1491e-02,\n",
       "        -1.2280e-02,  2.3577e-02,  6.6729e-03,  2.7811e-02, -6.1744e-02,\n",
       "         3.0685e+00, -1.8870e-02,  1.8418e-02, -5.8950e-04, -3.9127e-02,\n",
       "        -1.2742e-02, -3.3234e-02,  4.1388e-03, -2.1280e-01,  3.9684e-02,\n",
       "         2.4262e-02,  1.7842e-01, -4.1642e-04, -5.5655e-02, -3.9652e-02,\n",
       "         3.6324e-02, -3.6439e-04,  3.3898e-02,  3.8444e-02,  3.1045e-02,\n",
       "         1.6215e-01,  1.9416e-02,  3.9075e-02, -7.9726e-02, -3.5166e-02,\n",
       "        -6.1327e-02, -9.5163e-02, -1.3732e-02, -2.8533e-02, -3.2208e-02,\n",
       "         5.1764e-02,  5.1271e-02,  1.3170e-02, -2.8410e-02,  1.9225e-02,\n",
       "        -6.1447e-02,  9.4512e-02,  1.7898e-02,  3.3249e-02, -8.4298e-02,\n",
       "        -1.6876e-03,  2.2678e-02,  3.8959e-02, -2.6315e-02,  2.6722e-02,\n",
       "        -4.1036e-02, -4.3836e-02,  7.2811e-03,  2.3958e-02,  2.5368e-02,\n",
       "         7.1948e-02, -2.8513e-01,  6.8697e-03, -3.6890e-02, -2.6570e-03,\n",
       "        -2.4861e-02, -2.1709e+00, -7.5174e-02, -3.7429e-02,  4.7887e-02,\n",
       "         2.3458e-02, -9.0910e-01,  2.8369e-01, -7.7521e-03,  2.6658e-02,\n",
       "        -5.5254e-01, -2.5945e-02, -2.3110e-02,  3.5670e-02, -3.8938e-02,\n",
       "         6.3535e-02,  2.6753e-02,  3.8857e-02, -7.1311e-03, -5.0512e-01,\n",
       "        -8.4332e-02, -8.9942e-03, -9.6385e-01, -1.9003e-01, -1.6461e-02,\n",
       "         1.5758e-02, -2.9520e-02, -3.7219e-02,  2.6768e-02,  7.8288e-02,\n",
       "         3.6225e-02, -2.4597e-02, -9.8554e-03,  1.7192e-02,  2.0096e-01,\n",
       "         4.9101e-02,  1.9576e-02, -4.3461e-03,  9.6674e-02, -2.8922e-02,\n",
       "         1.3446e-02, -1.8345e-02,  5.6624e-02,  1.7636e-02,  8.8476e-03,\n",
       "        -2.3983e-02, -4.1040e-02, -3.5934e-01,  1.3422e-02, -1.9759e-02,\n",
       "         5.2325e-02, -2.6986e-02,  5.4985e-03, -1.2450e-01, -1.7049e-03,\n",
       "         3.3860e-04, -4.5541e-02, -2.4598e-02, -1.8265e-02, -7.3527e-03,\n",
       "         5.3070e-02,  2.5912e-02, -4.8607e-02,  3.5768e-02, -2.2845e-02,\n",
       "        -3.4739e-01, -3.7481e-02, -8.0708e-03,  7.6596e-02, -4.2530e-02,\n",
       "         4.2420e-02,  4.8602e-02,  2.1992e-02,  1.1385e-01, -2.4760e-02,\n",
       "        -9.0741e-02,  1.1689e-02,  9.7670e-02,  6.2420e-02,  8.3420e-04,\n",
       "         5.7120e-02, -3.3638e-03,  4.3362e-02,  6.4554e-03,  4.3409e-01,\n",
       "        -2.8289e-02,  8.4534e-03,  1.6225e-01,  2.1655e-02,  5.4778e-02,\n",
       "         3.3466e-02,  1.0512e-02,  6.3881e-03, -1.7619e-02, -3.7032e-02,\n",
       "        -5.3927e-02, -2.1898e-02,  2.0860e-02,  4.2955e-01,  2.8156e-02,\n",
       "        -3.0496e-01, -5.0267e-02,  3.3228e-02, -1.9458e-03,  4.3604e-01,\n",
       "         1.2594e-02,  1.6785e-03,  3.2018e-02, -1.1435e-02,  3.0255e-02,\n",
       "        -2.8112e-02,  4.1541e-03, -2.9137e-02, -5.2214e-02, -7.5817e-02,\n",
       "        -5.0438e-03,  3.8585e-03,  9.9854e-02, -2.6922e-02,  9.8854e-03,\n",
       "        -7.0485e-03, -5.8754e-03,  2.7603e-02,  9.0747e-02, -8.6773e-01,\n",
       "        -2.5373e-02,  9.3951e-03,  4.5241e-03,  2.1289e-02, -1.7936e-02,\n",
       "        -3.6137e-02, -2.0088e-02, -1.9245e-02,  1.3789e-01,  3.2350e-02,\n",
       "         4.9784e-02,  7.1033e-03,  2.4016e-02, -4.6873e-04,  1.0043e-02,\n",
       "        -1.8570e-02, -1.9425e-01,  2.0531e-01,  4.4722e-02, -4.8765e-02,\n",
       "         3.0511e-02,  1.5151e-04,  1.1109e-02, -1.5149e+00,  2.5688e-02,\n",
       "         7.3238e-03,  4.4182e-02, -1.5863e-02, -6.9036e-02,  9.6258e-03,\n",
       "         2.1722e-02,  3.8190e-01,  1.3655e-02, -3.3429e-02,  3.3138e-02,\n",
       "        -1.7596e-02,  1.2354e-02, -9.8003e-02,  1.7169e-02,  3.9591e-02,\n",
       "         4.0094e-02, -3.8441e-02,  7.1837e-02, -3.8313e-03, -3.9395e-02,\n",
       "        -2.5580e-01, -4.1298e-02, -2.0359e-01, -9.5320e-01,  2.5642e-02,\n",
       "         9.2723e-03,  3.7339e-02,  3.6342e-02, -7.0528e-03, -6.5394e-02,\n",
       "         1.2265e-02,  1.0122e-02,  1.1549e-02, -1.8140e-02,  6.1482e-03,\n",
       "         1.7135e+00, -3.5519e-02, -5.0767e-02,  7.6745e-03,  2.9331e-02,\n",
       "         3.0430e-03, -3.1479e-02, -1.2568e-02, -2.1320e-02,  2.1707e-02,\n",
       "         1.8987e-02, -8.8883e-01,  1.2125e-02,  2.6438e-02,  2.0943e-01,\n",
       "         3.2970e-02,  6.6763e-02,  3.3256e-02, -3.2825e-02, -1.2005e-01,\n",
       "        -4.9361e-02,  5.1311e-02, -3.3480e-02,  9.5489e-03,  6.4813e-02,\n",
       "        -2.4486e-02, -2.7666e-01,  4.9428e-04,  4.4311e-02, -1.4237e-02,\n",
       "        -1.6418e-02,  1.8140e-02,  8.0819e-02,  1.3806e-02,  2.2750e-03,\n",
       "        -2.3704e-02,  3.5261e-02,  7.6599e-03,  3.8602e-02,  6.6556e-03,\n",
       "        -1.7804e-02,  1.3549e+00,  1.9752e-02, -3.4907e-02,  6.4030e-03,\n",
       "        -6.0518e-02,  6.0842e-02, -1.7444e-02,  1.0880e-01, -1.8750e-02,\n",
       "        -1.0402e-02, -6.6880e-03, -7.7606e-03, -2.2134e-02,  1.2712e-02,\n",
       "        -1.2865e-02, -3.5302e-02, -1.6899e-03,  6.8769e-03,  2.6735e-02,\n",
       "         3.6005e-02,  2.0541e-03, -4.8858e-02, -5.8962e-02, -4.0852e-02,\n",
       "        -1.1628e-02,  1.2221e-02, -1.7475e-02,  2.4019e-01, -2.5707e-02,\n",
       "         1.1639e-02, -7.5646e-03, -2.8997e-03, -1.0732e-02, -1.1266e-01,\n",
       "         1.8306e-01, -9.5816e-03,  9.0851e-02, -4.1114e-02, -4.6769e-02,\n",
       "        -1.4158e-02,  2.6344e-02,  6.5998e-01, -5.0159e-01,  6.4231e-02,\n",
       "         6.2835e-03, -3.3330e-02, -1.6098e+00,  3.8070e-02,  1.1026e-02,\n",
       "        -2.5894e-02,  6.1403e-03,  4.6618e-02,  2.7380e-02, -3.2654e-02,\n",
       "         2.3865e-02, -2.8636e-02,  2.9206e-03, -3.3496e-03,  2.7162e-02,\n",
       "         1.9196e-01, -6.2142e-02,  2.1860e-02,  2.7283e-01,  3.2564e-02,\n",
       "         9.9084e-03, -3.1970e-02,  3.6526e-02,  2.2737e-02, -1.8527e-01,\n",
       "        -8.3985e-03, -3.7638e-02,  2.2447e-02,  2.3149e-02,  4.9008e-03,\n",
       "        -4.9940e-01, -5.8577e-02,  1.3585e-02, -5.3179e-02,  8.9382e-03,\n",
       "        -2.9503e-02,  1.1848e-02, -6.9815e-03,  1.5438e-02, -6.4257e-02,\n",
       "        -8.2343e-02, -8.6457e-02, -7.3714e-02,  1.6764e-02,  6.2733e-02,\n",
       "         3.0075e+00,  2.3481e-03, -8.1105e-03,  3.6314e-02, -4.5381e+00,\n",
       "         3.0046e-02, -4.3590e-03,  1.9305e-03, -1.9914e-02, -1.3940e+00,\n",
       "         8.5825e-02,  1.5078e-03, -3.5813e-02, -2.5510e-02,  8.1946e-02,\n",
       "        -3.9880e-02, -4.3724e-03,  4.6706e-02, -7.5673e-02, -5.7956e-02,\n",
       "        -4.0163e-02,  8.6380e-03, -1.8892e-02,  1.2066e-02,  7.1348e-02,\n",
       "         3.0804e-02,  3.5332e-02, -3.0011e-02, -4.6266e-03, -3.0416e-01,\n",
       "         7.3280e-03, -3.6848e-01,  1.8192e-02,  2.1566e-02,  2.4931e-02,\n",
       "        -3.1073e-02,  9.5428e-05, -1.1928e-01, -1.5507e-03, -2.4268e-01,\n",
       "        -8.4555e-03, -1.3348e-02,  1.5365e-03,  6.2889e-03,  1.3201e-02,\n",
       "         1.5080e-02, -3.1683e-03,  6.8163e-02, -7.9179e-01, -4.6202e-03,\n",
       "         9.1074e-01, -1.6140e-01,  1.3297e-02,  6.4707e-03,  1.4513e+00,\n",
       "         5.0443e-02,  1.4084e-01,  1.5878e-02,  3.4399e-01, -8.5022e-03,\n",
       "        -5.8426e-03, -1.0584e-02,  5.1128e-02,  3.6588e-02,  3.8108e-02,\n",
       "        -4.0882e-02, -3.5466e-02, -1.0839e-02,  2.4481e-03, -2.0107e-01,\n",
       "         6.0284e-03,  4.9156e-02, -2.1477e-02, -2.4268e-03, -9.1521e-03,\n",
       "         3.4912e-01, -1.5139e-01, -7.5483e-02,  2.2206e-02, -6.1442e-03,\n",
       "        -6.3152e-03,  9.7740e-02,  1.3022e-02,  1.3445e-02,  2.6431e-02,\n",
       "        -3.6497e-02, -1.5883e+00, -6.3216e-02, -4.6221e-02, -2.8667e-02,\n",
       "         5.4453e-02, -5.2860e-02,  2.5602e-03, -8.4281e-03,  3.3671e-03,\n",
       "        -4.3044e-02,  2.8267e-02,  5.4490e-02], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.wpe(torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encodings.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
