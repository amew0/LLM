{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnas = \" \\n\\n### 1\\nQ: What is the patient's liver condition based on the provided lab results?\\nM: cirrhosis, chronic hepatitis, liver damage, liver failure\\nA: chronic hepatitis\\n\\n### 2\\nQ: What is the patient's serum albumin level?\\nM: 3.4, 7.54, 11.3, 15.6\\nA: 3.4\\n\\n### 3\\nQ: Which of the following is a recommendation for the patient's treatment?\\nM: avoid fatty diet, take beta blocker, take more sugar cane juice, consult gastroenterologist\\nA: consult gastroenterologist\\n\\n### 4\\nQ: What is the patient's bilirubin level?\\nM: 2.38, 4.88, 17.16, 25.8\\nA: 17.16\\n\\n### 5\\nQ: What is the patient's creatinine level?\\nM: 2.5, 4.88, 7.5, 10.5\\nA: 4.88\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = qnas.find(\"### 1\")\n",
    "\n",
    "qna_section = qnas[start_index:].strip()\n",
    "\n",
    "# Skip the empty string before the first ###\n",
    "qna_blocks = qna_section.split(\"### \")[1:]\n",
    "\n",
    "\n",
    "qnas_formatted = []\n",
    "for block in qna_blocks:\n",
    "    lines = block.strip()\n",
    "    q = lines[lines.find(\"Q: \"):lines.find(\"A: \")].strip()\n",
    "    a = lines[lines.find(\"A: \"):].strip()\n",
    "    \n",
    "    qnas_formatted.append({\"Q\": q, \"A\":a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnas_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "from scipy.stats import pearsonr\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from time import time\n",
    "import gc\n",
    "import json\n",
    "import yaml\n",
    "import argparse\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import fire\n",
    "import inspect\n",
    "\n",
    "\n",
    "logg = lambda x: print(f\"------------------------ {x} ---------------------------\")\n",
    "\n",
    "\n",
    "def inspectt(frame):\n",
    "    logg(\"\")\n",
    "    args, _, _, values = inspect.getargvalues(frame)\n",
    "    for arg in args:\n",
    "        print(f\"\\t{arg}: {values[arg]}\")\n",
    "    logg(\"\")\n",
    "\n",
    "def get_prompts_from_template(filepath, name, eval_name):\n",
    "    default_config = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "\n",
    "    candidate_prompt = data[name][\"candidate_prompt\"]\n",
    "    evaluator_prompt = data[eval_name][\"evaluator_prompt\"]\n",
    "    candidate_generation_config = data[name].get(\"candidate_generation_config\", default_config)\n",
    "    evaluator_generation_config = data[eval_name].get(\n",
    "        \"evaluator_generation_config\", default_config\n",
    "    )\n",
    "\n",
    "    print(\"candidate_prompt: \", candidate_prompt)\n",
    "    print(\"evaluator_prompt: \", evaluator_prompt)\n",
    "    print(\"candidate_generation_config: \", candidate_generation_config)\n",
    "    print(\"evaluator_generation_config: \", evaluator_generation_config)\n",
    "\n",
    "    return (\n",
    "        candidate_prompt,\n",
    "        evaluator_prompt,\n",
    "        candidate_generation_config,\n",
    "        evaluator_generation_config,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_tokenizer_and_model(model_name: str, cache_dir: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=f\"{cache_dir}/tokenizer\",\n",
    "        pad_token_id=0,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=f\"{cache_dir}/model\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        offload_buffers=True,\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def tokenize(prompt, tokenizer):\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(batch, tokenizer, prompt_template):\n",
    "    # print(batch)\n",
    "    prompts = [prompt_template.format(d[0], d[1]) for d in zip(batch[\"instruction\"], batch[\"input\"])]\n",
    "    print(\"a\", prompts)\n",
    "    tokenized_prompts = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "    print(\"b\", tokenized_prompts)\n",
    "    return tokenized_prompts\n",
    "\n",
    "\n",
    "def eval_prompt_tokenizer(generated, output, eval_tokenizer, prompt=None):\n",
    "    prompt = prompt.format(generated, output)\n",
    "    tokenized_full_prompt = tokenize(prompt, tokenizer=eval_tokenizer)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "\n",
    "def extract_score(text):\n",
    "    match = re.search(r\"\\b\\d+\\.\\d+\\b\", text)\n",
    "    return float(match.group(0)) if match else -1.0\n",
    "\n",
    "\n",
    "def log2json(results, json_result):\n",
    "    with open(json_result, \"w\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def generate_response(model, tokenizer, input_ids, attention_mask, generation_config):\n",
    "    # torch.LongTensor(input_ids).to(model.device)\n",
    "    # torch.LongTensor(attention_mask).to(model.device)\n",
    "    # try:\n",
    "        output = model.generate(\n",
    "            input_ids=torch.stack(input_ids).to(model.device),\n",
    "            attention_mask=torch.stack(attention_mask).to(model.device),\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            **generation_config,\n",
    "        )\n",
    "        print(output[0])\n",
    "        response_ids = output[0][len(input_ids[0]) :]\n",
    "        response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "        return response, output\n",
    "    # except RuntimeError as e:\n",
    "    #     if \"inf\" in str(e) or \"nan\" in str(e):\n",
    "    #         print(f\"Skipping example due to invalid output: {e}\")\n",
    "    #         return None\n",
    "    #     else:\n",
    "    #         raise \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=f\"./out\"\n",
    "cache_dir=f\"/dpc/kunf0097/l3-8b\"\n",
    "eval_data_path=\"./data/1/eval_sample.json\"\n",
    "log_file=None\n",
    "name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "eval_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "run_id=datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "log2wandb: bool = True\n",
    "project=\"huggingface\"\n",
    "entity=\"my-ku-org\"\n",
    "evals_per_example=2\n",
    "batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "evaluator_name=\"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    candidate_prompt,\n",
    "    evaluator_prompt,\n",
    "    candidate_generation_config,\n",
    "    evaluator_generation_config,\n",
    ") = get_prompts_from_template(\"template.yaml\", candidate_name, evaluator_name)\n",
    "\n",
    "\n",
    "if log2wandb and (project is None or entity is None):\n",
    "    raise ValueError(\"Both 'project' and 'entity' must be set if 'log2wandb' is True.\")\n",
    "\n",
    "if log_file is None:\n",
    "    log_file = f\"{output_dir}/results_{name.split('/')[1]}_{run_id}.json\"\n",
    "\n",
    "inspectt(inspect.currentframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator_tokenizer, evaluator_model = get_tokenizer_and_model(\n",
    "#     model_name=eval_name, cache_dir=cache_dir\n",
    "# )\n",
    "\n",
    "candidate_tokenizer, candidate_model = get_tokenizer_and_model(\n",
    "    model_name=name, cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# candidate_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#         name,\n",
    "#         cache_dir=f\"{cache_dir}/tokenizer\",\n",
    "#         pad_token_id=0,\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_tokenizer.pad_token = candidate_tokenizer.bos_token \n",
    "candidate_tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=eval_data_path)\n",
    "eval_dataset = data[\"train\"].map(\n",
    "    lambda x: generate_and_tokenize_prompt(x, candidate_tokenizer, candidate_prompt),\n",
    "    batched=True,  # Process in batches\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_eval_dataset  =torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batched_eval_dataset :\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(batch[\"input_ids\"])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(batch[\"attention_mask\"])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidate_tokenizer.decode(torch.stack(batch[\"input_ids\"])[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, output = generate_response(\n",
    "    candidate_model,\n",
    "    candidate_tokenizer,\n",
    "    batch[\"input_ids\"],\n",
    "    batch[\"attention_mask\"],\n",
    "    candidate_generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidate_tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enshiallah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"out/results_240623023136_240628153415.json\", \"r\") as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "table = wandb.Table(columns=list(results[0].keys()))\n",
    "for r in results:\n",
    "    table.add_data(*r.values())\n",
    "run =wandb.init(project=\"huggingface\", entity=\"my-ku-org\", name=\"laaj-llama-3-8b-medical-v240623023136\")\n",
    "wandb.log({\"Evaluation Results\": table})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evals **MMLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =load_dataset(\"cais/mmlu\", \"clinical_knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = data[\"dev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import inspect\n",
    "import os\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import fire\n",
    "import huggingface_hub\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from utils.eval_helper import inspectt, logg\n",
    "from transformers import TrainerCallback\n",
    "# from utils.ft_helper import generate_and_tokenize_prompt, dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./out\"\n",
    "cache_dir = f\"/dpc/kunf0097/l3-8b\"\n",
    "train_data_path = \"./data/medical-36-row.json\"\n",
    "model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_save_path: str = None\n",
    "chpt_dir: str = None\n",
    "# run_id = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "run_id = '240714024529'\n",
    "\n",
    "if model_save_path is None:\n",
    "    model_save_path = f\"{cache_dir}/model/{model_name}-v{run_id}\"\n",
    "if chpt_dir is None:\n",
    "    chpt_dir = f\"{cache_dir}/chpt/{run_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = None\n",
    "if os.path.isdir(chpt_dir):\n",
    "    checkpoints = [d for d in os.listdir(chpt_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        last_checkpoint = os.path.join(chpt_dir, max(checkpoints, key=lambda cp: int(cp.split('-')[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, tokenizer, cutoff_len: int = None):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    result[\"input_ids\"] = result[\"input_ids\"].flatten()\n",
    "    result[\"attention_mask\"] = result[\"attention_mask\"].flatten()\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].clone()  # Clone input_ids for labels\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point, tokenizer, cutoff_len: int = None):\n",
    "    if cutoff_len is None:\n",
    "        tokenized_full_prompt = tokenize(data_point[\"prompt\"], tokenizer=tokenizer)\n",
    "    else:\n",
    "        tokenized_full_prompt = tokenize(\n",
    "            data_point[\"prompt\"], tokenizer=tokenizer, cutoff_len=cutoff_len\n",
    "        )\n",
    "    return tokenized_full_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52d7b03a8ab4eaaab09f0b656b018eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=f\"{cache_dir}/tokenizer\")\n",
    "\n",
    "# Initialize model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=f\"{cache_dir}/model\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    # return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 4096)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for LoRA training\n",
    "peft_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=train_data_path, split=\"train\") \n",
    "# not shuffling for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def reorder_dataset(dataset, start_index):\n",
    "    # Split the dataset into two parts: before and after the start index\n",
    "    dataset_part1 = dataset.select(range(start_index, len(dataset)))\n",
    "    dataset_part2 = dataset.select(range(start_index))\n",
    "    \n",
    "    # Concatenate the two parts to get the reordered dataset\n",
    "    reordered_dataset = concatenate_datasets([dataset_part1, dataset_part2])\n",
    "    return reordered_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebbdf1d07e34576a91101a621cca651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 1\n",
    "start_index = 0\n",
    "if last_checkpoint is not None:\n",
    "    start_index = (\n",
    "        int(last_checkpoint.split(\"-\")[-1])\n",
    "        * per_device_train_batch_size\n",
    "        * gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "data = load_dataset(\"json\", data_files=train_data_path, split=\"train\")\n",
    "data = reorder_dataset(data, start_index)\n",
    "\n",
    "cutoff_len = 296  # (75% of the data wont be affected)\n",
    "train_dataset = data.map(lambda x: generate_and_tokenize_prompt(x, tokenizer, cutoff_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it from .yaml\n",
    "train_args = SFTConfig(\n",
    "    run_name=f\"ft-{model_name.split('/')[1]}-{run_id}-v{last_checkpoint.split('-')[-1]}\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # only 1 is allowed on the no shuffler [needs revision]\n",
    "    eval_accumulation_steps=1,  # !very important to send data to cpu\n",
    "    warmup_steps=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=False,\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_torch\",\n",
    "    output_dir=f\"{chpt_dir}\",\n",
    "    group_by_length=False,\n",
    "    dataloader_drop_last=False,\n",
    "    save_steps=2,\n",
    "    save_total_limit=3,\n",
    "    max_seq_length=cutoff_len,\n",
    "    resume_from_checkpoint=last_checkpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, I had a subarachnoid bleed and coiling of brain aneurysm last year. I am having some major bilateral temple pain along with numbness that comes and goes in my left arm/hand/fingers. I have had headaches since the aneurysm, but this is different. Also, my moods have been horrible for the past few weeks.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(a)\n",
    "b['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c = next(a)\n",
    "c['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintExampleCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.example_iter = None  # Iterator for examples\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.example_iter = iter(kwargs['train_dataloader'])\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        # Get the next example from the iterator\n",
    "        try:\n",
    "            example = next(self.example_iter)\n",
    "        except StopIteration:\n",
    "            # Reinitialize the iterator if it's exhausted and get the next example\n",
    "            self.example_iter = iter(kwargs['train_dataloader'])\n",
    "            example = next(self.example_iter)\n",
    "\n",
    "        # Decode and print the example\n",
    "        print(f\"Step {state.global_step}: {tokenizer.decode(example['input_ids'][0], skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "import datasets\n",
    "from transformers.trainer_utils import seed_worker\n",
    "\n",
    "class SFTTrainerNoShuffle(SFTTrainer):\n",
    "    def _get_train_sampler(self):\n",
    "\n",
    "        # Always use SequentialSampler to prevent shuffling\n",
    "        return SequentialSampler(self.train_dataset)\n",
    "        \n",
    "\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the training DataLoader.\n",
    "\n",
    "        Will use no sampler if `train_dataset` does not implement `__len__`, a sequential sampler otherwise.\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "        if isinstance(train_dataset, datasets.Dataset):\n",
    "            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "        else:\n",
    "            data_collator = self._get_collator_with_removed_columns(\n",
    "                data_collator, description=\"training\"\n",
    "            )\n",
    "\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self._train_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "            \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
    "        }\n",
    "\n",
    "        if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "            dataloader_params[\"sampler\"] = self._get_train_sampler()\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "            dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "            dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n",
    "\n",
    "        return self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainerNoShuffle(\n",
    "# trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    args=train_args,\n",
    "    callbacks=[PrintExampleCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'240714024529'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamew0\u001b[0m (\u001b[33mmy-ku-org\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kunet.ae/ku5001069/LLM/wandb/run-20240714_030048-4st4csqe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/my-ku-org/huggingface/runs/4st4csqe' target=\"_blank\">ft-Meta-Llama-3-8B-Instruct-v240714024529</a></strong> to <a href='https://wandb.ai/my-ku-org/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/my-ku-org/huggingface' target=\"_blank\">https://wandb.ai/my-ku-org/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/my-ku-org/huggingface/runs/4st4csqe' target=\"_blank\">https://wandb.ai/my-ku-org/huggingface/runs/4st4csqe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: system If you are a doctor, please answer the medical questions based on the patient's description.user This is the question: I had a alt reading about 2+ months ago of 43 then retested today and got a 60 alt reading my other readings came back normal should I be concerned?Im 40 years 5ft 3 and weigh 192albs have a history of chronic vertigo and insomnia, and chronic sinus problems. I take valium 5 mg 4 times a day, fluticansone 2 puffs each nose once a day, qvair two puffs twice a day to prevent weasing, claritin for allergies, and I do sinus rinses as needed. I am female.My doctor says none of the medicine has liver side effects, and I dont drink alcohol but a sip on the rare occasion. He thinks it could be do my weight and Im working on this with gradual exercise when Im able too. Should I be worried with this test result? I had one other result that was high my triglcyerides read 198? WWW.WWWW.WWassistant Hello and thank you for asking Chat Doctor. I have read your report, and I understand your concerns. First I would say to not worry. Normal Alt levels are 7 to 56 unit (depending on from the kits that laboratories use)In one of your tests, your Alt was \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/54 00:29 < 02:34, 0.24 it/s, Epoch 0.89/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.446400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.958300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.287100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.265800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: system If you are a doctor, please answer the medical questions based on the patient's description.user This is the question: Hi, I had a subarachnoid bleed and coiling of brain aneurysm last year. I am having some major bilateral temple pain along with numbness that comes and goes in my left arm/hand/fingers. I have had headaches since the aneurysm, but this is different. Also, my moods have been horrible for the past few weeks.assistant Aneurysm in brain causes headache due to compression of pain sensitive structure in brain. But pain is usually unilateral not bilateral. This pain will not radiate in both upper limb. Bilateral upper limb pain may be due to cervical spondylosis or other causes. You have anxiety of aneurysm, so due to anxiety your mood is horrible because aneurysm does not cause horrible mood change. So your headache may be chronic tension type headache. You may relieve by antianxiety Chat Doctor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: system If you are a doctor, please answer the medical questions based on the patient's description.user This is the question: for the pst  6 days  i have been haing  upper  abdome  distress...  i have had gall bladder  surgery  2 yrs  ago..in the past  2  yrs  when i get these bouts .. i go to  emerg  they run cardiograms  xrays,blood work,all comes back normal ...im stressing out  over this  as im  scared every time it happens  im having a heart  attack  wht is it ?assistant Hi. Thanks for your query, read and understood your problems. You are getting bouts of pain in the upper abdomen within the last 2 years particularly after the gall bladder surgery. This is great news that cardiac problems are ruled out by the tests every time in the emergency Room. This means that we have to find a local cause in the abdomen itself. I would suggest the following\n",
      "Step 11: system If you are a doctor, please answer the medical questions based on the patient's description.user This is the question: Dear Doctor, We are trying to have a baby and gave it a big try this time but as we are planning to go for an Invitro (ICSI) for the next period, my doctor gave me Primolut Not from day 16 to day 25. I am wondering if this will have a negative impact on my pregnancy if I am pregnant because as explained we gave it a big try this time. Thank you in advance for your response. With my Best Regards; Mme Sueassistant Hellos you are going in for CSI next month, your doctor has given you Primal N for regulating the cycles. However, in case you have conceived this month, it is not advisable to take Primal N, as it causes virilization of female fetus and other abnormalities in the baby. There are other formulations of progesterone like Me Chat Doctor. These can be substituted for Primal - N. Please brief your IVF practitioner about the state of affairs and switch over to a safer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12: system If you are a doctor, please answer the medical questions based on the patient's description.user This is the question: Hi hope you can helpive been getting chest pains, not severe but a constant pain, can happen when im just resting but has also happened when im exercising, it continues through to my back as well. I sometimes wake up with chest pains too. i have a strong history of heart disease. grandfather died at 26 from a heat attack and mother had one at 45, also grandmother on the other side had a heart attack. Could you please explain what is happening? Thanksassistant HelloThanks for posting at Chat Doctor. You have not provided your age on the details. I would have been able to help u better had you mentioned your age. You have complaints of chest pain radiating to back with strong family history. So in your case it is better that we evaluate you for presence of heart disease. I recommend an ECG and 2d echo initially. If both these tests are normal you can proceed for a treadmill test which requires you to walk on a treadmill while your ECG is continuously monitored. This will show if the heart blood supply suffers while you are exercising which indirectly predicts whether you have blockages in your heart. Please do the above tests and revert. Wishing you good healthRegards.\n",
      "Step 13: system If you are a doctor, please answer the medical questions based on the patient's description.user This is the question: I too fell and hurt my left side of my chest. I have a large lump and I am very black and blue. I need to fly out on Sunday this fall happen last Sunday. I have been taking aleve for pain and arnica gel. Should I be worried about the flight? I am 70 Thank you,assistant Welcome to ChatDoctorYou may be having rib fractures. You have to do x-ray chest as early as possible. Rib fractures can cause pneumothorax/air in chest or hemothorax/blood in the chest which will be seen on x-ray. Only rib fractures needs no treatment except painkillers. But if pseudo or hemothorax is present that needs ICD tube insertion in the chest. In that case air travel should be avoided. Regards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14: system If you are a doctor, please answer the medical questions based on the patient's description.user This is the question: my mother is 80 yrs. detected with vulva cancer. vulva is swollen red,around discolourisation, white growth now extending to rectum. biopsy shows keratinizing squamous carcinoma vulva(welldifferentiated,invasive).she is little asthmatic, has cervical spondeolytis.assistant Hi, dairy have gone through your question. I can understand your concern. She has well differentiated keratinizing squamous cell carcinoma.  If her general health is good then treatment of choice is wide excision of carcinoma followed by chemotherapy or radiotherapy if needed. Consult your doctor and take treatment accordingly. Hope I have answered your question, if you have doubt then I will be happy to answer. Thanks for using Chat Doctor. Wish you a very good health.\n",
      "Step 15: system If you are a doctor, please answer the medical questions based on the patient's description.user This is the question: Hey I am on my third antibiotic and still coughing and my ribs are killing me especially right side. Doc claims it is viral cough syndrome left over from bronchitis. I do not think so. No fever the whole time. Xray was clear on lungs. Taking mucinex, third antibiotic, inhaler and can not get better. Ulcers in mouth and my face in breaking out bad. Been sick since December 28,2012.assistant Hi welcome to Chat Doctor forum. The reasons for persistent cough, without findings in the lungs might, Uveitis a growth at the end of the throat, by touching the pharynx cause severe cough. For this type of cough you need to use cough suppressants like dexametarphane, codeine, etc which are present in some cough syrups.  Best thing to get relief immediately is Darling with Luke warm salt water 2 or 3 times a day. Avoid taking fruits, juices, ice creams, ice water to Chat Doctor.  I advise you to consult an E.N.T.surgeon who is the doctor concerned with the causes in the pharynx.  Wishing for a quick and comfortable relief, and thanks for calling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunet.ae/ku5001069/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/kunet.ae/ku5001069/.conda/envs/torch20/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:440\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 440\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2291\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2732\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2729\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2732\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2811\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2809\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   2810\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 2811\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   2814\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   2815\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3355\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3355\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3357\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3432\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3430\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   3431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3432\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3433\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[1;32m   3434\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/.conda/envs/torch20/lib/python3.10/site-packages/peft/peft_model.py:286\u001b[0m, in \u001b[0;36mPeftModel.save_pretrained\u001b[0;34m(self, save_directory, safe_serialization, selected_adapters, save_embedding_layers, is_main_process, convert_pissa_to_lora, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_pissa_to_lora \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m         output_state_dict \u001b[38;5;241m=\u001b[39m save_pissa_as_lora(\n\u001b[1;32m    284\u001b[0m             peft_config, convert_pissa_to_lora, output_state_dict, kwargs\n\u001b[1;32m    285\u001b[0m         )\n\u001b[0;32m--> 286\u001b[0m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAFETENSORS_WEIGHTS_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_main_process:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_pissa_to_lora \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/safetensors/torch.py:284\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    254\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    255\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    256\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    257\u001b[0m ):\n\u001b[1;32m    258\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(last_checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
